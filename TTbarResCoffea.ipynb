{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TTbarResCoffea` Notebook to perform the data-driven mistag-rate-based ttbar hadronic analysis. \n",
    "This module must be run twice: \n",
    "   1. Make the mistag rate in the \"anti-tag and probe\" selection \n",
    "and the expectation in the signal region from MC,\n",
    "   1. Applies that mistag rate and the mod-mass procedure to the single-tag selection. \n",
    "\n",
    "These are all done in bins of\n",
    "b-tag categories (0, 1, $\\ge 2$) and rapidity ($|y| \\le 1.0$, $|y| > 1.0$).\n",
    "The signal region is two top-tagged jets. \n",
    "The background estimate is the single-tag selection weighted by the mistag rate from the\n",
    "\"anti-tag and probe\" region, with the mass of the weighted jet set to a random\n",
    "value from QCD MC in the 1-ttag region. \n",
    "\n",
    "\n",
    "The preselection is:\n",
    "- AK4-based $H_{T} > 1100$ GeV (to be on the trigger plateau). \n",
    "- $\\ge 2$ AK8 jets with AK8 $p_{T} > 400$ GeV and $|y| < 2.5$, loose jet ID applied from matched AK4 jets\n",
    "\n",
    "The 1-tag selection adds:\n",
    "- $\\ge 1$ AK8 jet with top tagging applied to randomly-assigned tag jet. \n",
    "\n",
    "\n",
    "The anti-tag selection is disjoint from the 1-tag selection:\n",
    "- $\\ge 1$ AK8 jet with top tagging VETO applied to randomly-assigned tag jet. \n",
    "\n",
    "\n",
    "The 2-tag selection is:\n",
    "- $\\ge 2$ AK8 jets with top tagging applied to both leading jets. \n",
    "\n",
    "\n",
    "The ttbar candidate mass assumes the two leading top-tagged jets are the top quarks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor\n",
    "from coffea import util\n",
    "from awkward import JaggedArray\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrootdstr1 = 'root://cmseos.fnal.gov//'\n",
    "xrootdstr2 = 'root://cmsxrootd.fnal.gov//'\n",
    "xrootdstr3 = 'root://cmsxrootd-site.fnal.gov/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcdfilename = 'QCD.txt'\n",
    "with open(qcdfilename) as f:\n",
    "    qcdfiles = [xrootdstr2 + s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttbarfilename = 'TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8.txt'\n",
    "with open(ttbarfilename) as f:\n",
    "    ttbarfiles = [xrootdstr2 + s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetdatafilename = 'JetHT_Data.txt'\n",
    "with open(jetdatafilename) as f:\n",
    "    #jetdatafiles = [xrootdstr2 + s.strip() for s in f.readlines() if \"/store/data/Run2017\" in s]\n",
    "    jetdatafiles = [xrootdstr2 + s.strip() for s in f.readlines()[::3]] # Every third datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(jetdatafiles[2]) # Test to see if correct files are collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from columnservice.client import ColumnClient\n",
    "cc = ColumnClient(\"coffea-dask.fnal.gov\")\n",
    "client = cc.get_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"@TTbarResAnaHadronic Package to perform the data-driven mistag-rate-based ttbar hadronic analysis. \n",
    "\"\"\"\n",
    "class TTbarResProcessor(processor.ProcessorABC):\n",
    "    def __init__(self, prng, htCut=1100., minMSD=105., maxMSD=210., tau32Cut=0.65, ak8PtMin=400., bdisc=0.7,\n",
    "                writePredDist=True,isData=True,year=2019, UseLookUpTables=False,\n",
    "                lu = None):\n",
    "        \n",
    "        self.prng = prng\n",
    "        self.htCut = htCut\n",
    "        self.minMSD = minMSD\n",
    "        self.maxMSD = maxMSD\n",
    "        self.tau32Cut = tau32Cut\n",
    "        self.ak8PtMin = ak8PtMin\n",
    "        self.bdisc = bdisc\n",
    "        self.writePredDist = writePredDist\n",
    "        self.writeHistFile = True\n",
    "        self.isData = isData\n",
    "        self.year=year\n",
    "        self.UseLookUpTables = UseLookUpTables\n",
    "        self.lu = lu # Look Up Tables\n",
    "        \n",
    "        self.ttagcats = [\"At\",\"at\", \"0t\", \"1t\", \"2t\"] #anti-tag+probe, anti-tag, 0, 1, >=2 ttags\n",
    "        self.btagcats = [\"0b\", \"1b\", \"2b\"]   # 0, 1, >=2 btags\n",
    "        self.ycats = ['cen', 'fwd']          # Central and forward\n",
    "        # Combine categories like \"0bcen\", \"0bfwd\", etc:\n",
    "        self.anacats = [ t+b+y for t,b,y in itertools.product( self.ttagcats, self.btagcats, self.ycats) ]\n",
    "        print(self.anacats)\n",
    "        \n",
    "        dataset_axis = hist.Cat(\"dataset\", \"Primary dataset\")\n",
    "        cats_axis = hist.Cat(\"anacat\", \"Analysis Category\")\n",
    "        \n",
    "        jetmass_axis = hist.Bin(\"jetmass\", r\"Jet $m$ [GeV]\", 50, 0, 500)\n",
    "        jetpt_axis = hist.Bin(\"jetpt\", r\"Jet $p_{T}$ [GeV]\", 50, 0, 5000)\n",
    "        ttbarmass_axis = hist.Bin(\"ttbarmass\", r\"$m_{t\\bar{t}}$ [GeV]\", 50, 0, 5000)\n",
    "        jeteta_axis = hist.Bin(\"jeteta\", r\"Jet $\\eta$\", 50, -5, 5)\n",
    "        jetphi_axis = hist.Bin(\"jetphi\", r\"Jet $\\phi$\", 50, -np.pi, np.pi)\n",
    "        jety_axis = hist.Bin(\"jety\", r\"Jet $y$\", 50, -3, 3)\n",
    "        jetdy_axis = hist.Bin(\"jetdy\", r\"Jet $\\Delta y$\", 50, 0, 3)\n",
    "        jetp_axis = hist.Bin(\"jetp\", r\"Jet $p$ [GeV]\", 50, 0, 7000)\n",
    "        manual_axis = hist.Bin(\"Jetp\", r\"Jet Momentum [GeV]\", [400, 500, 600, 800, 1000, 1500, 2000, 3000, 7000, 10000])\n",
    "        \n",
    "        subjetmass_axis = hist.Bin(\"subjetmass\", r\"SubJet $m$ [GeV]\", 50, 0, 500)\n",
    "        subjetpt_axis = hist.Bin(\"subjetpt\", r\"SubJet $p_{T}$ [GeV]\", 50, 0, 2000)\n",
    "        subjeteta_axis = hist.Bin(\"subjeteta\", r\"SubJet $\\eta$\", 50, -4, 4)\n",
    "        subjetphi_axis = hist.Bin(\"subjetphi\", r\"SubJet $\\phi$\", 50, -np.pi, np.pi)\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'ttbarmass': hist.Hist(\"Counts\", dataset_axis, cats_axis, ttbarmass_axis),\n",
    "            'jetmass':   hist.Hist(\"Counts\", dataset_axis, cats_axis, jetmass_axis),\n",
    "            'jetpt':     hist.Hist(\"Counts\", dataset_axis, cats_axis, jetpt_axis),\n",
    "            'jeteta':    hist.Hist(\"Counts\", dataset_axis, cats_axis, jeteta_axis),\n",
    "            'probept':     hist.Hist(\"Counts\", dataset_axis, cats_axis, jetpt_axis),\n",
    "            'probep':     hist.Hist(\"Counts\", dataset_axis, cats_axis, manual_axis),\n",
    "\n",
    "            'jetphi':    hist.Hist(\"Counts\", dataset_axis, cats_axis, jetphi_axis),\n",
    "            'jety':      hist.Hist(\"Counts\", dataset_axis, cats_axis, jety_axis),\n",
    "            'jetdy':     hist.Hist(\"Counts\", dataset_axis, cats_axis, jetdy_axis),\n",
    "            \n",
    "            'subjetmass':   hist.Hist(\"Counts\", dataset_axis, cats_axis, subjetmass_axis),\n",
    "            'subjetpt':     hist.Hist(\"Counts\", dataset_axis, cats_axis, subjetpt_axis),\n",
    "            'subjeteta':    hist.Hist(\"Counts\", dataset_axis, cats_axis, subjeteta_axis),\n",
    "            'subjetphi':    hist.Hist(\"Counts\", dataset_axis, cats_axis, subjetphi_axis),\n",
    "            \n",
    "            'numerator': hist.Hist(\"Counts\", dataset_axis, cats_axis, manual_axis),\n",
    "            'denominator': hist.Hist(\"Counts\", dataset_axis, cats_axis, manual_axis),\n",
    "            'cutflow': processor.defaultdict_accumulator(int),\n",
    "            \n",
    "        })\n",
    "\n",
    "            \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, df):\n",
    "        \n",
    "        output = self.accumulator.identity()\n",
    "        \n",
    "        #dataset = events.metadata['dataset']\n",
    "        dataset = df['dataset']\n",
    "        FatJets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nFatJet'],\n",
    "            pt=df['FatJet_pt'],\n",
    "            eta=df['FatJet_eta'],\n",
    "            phi=df['FatJet_phi'],\n",
    "            mass=df['FatJet_mass'],\n",
    "            msoftdrop=df['FatJet_msoftdrop'],\n",
    "            jetId=df['FatJet_jetId'],\n",
    "            tau1=df['FatJet_tau1'],\n",
    "            tau2=df['FatJet_tau2'],\n",
    "            tau3=df['FatJet_tau3'],\n",
    "            tau4=df['FatJet_tau4'],\n",
    "            n3b1=df['FatJet_n3b1'],\n",
    "            btagDeepB=df['FatJet_btagDeepB'],\n",
    "            deepTag_TvsQCD=df['FatJet_deepTag_TvsQCD'],\n",
    "            deepTagMD_TvsQCD=df['FatJet_deepTagMD_TvsQCD']\n",
    "            )\n",
    "        # ---- Define Jets ---- #\n",
    "        Jets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nSubJet'],\n",
    "            pt=df['Jet_pt'],\n",
    "            eta=df['Jet_eta'],\n",
    "            phi=df['Jet_phi'],\n",
    "            mass=df['Jet_mass']\n",
    "            )\n",
    "        \n",
    "        # ---- Get event weights from dataset ---- #\n",
    "        if 'JetHT' in dataset: # If data is used...\n",
    "            evtweights = np.ones(FatJets.size) # set all \"data weights\" to one\n",
    "        else: # if Monte Carlo dataset is used...\n",
    "            evtweights = df[\"Generator_weight\"].reshape(-1, 1).flatten()\n",
    "        \n",
    "        # ---- Show all events ---- #\n",
    "        output['cutflow']['all events'] += FatJets.size        \n",
    "        \n",
    "        # ---- Apply HT Cut ---- #\n",
    "        hT = Jets.pt.sum()\n",
    "        passhT = (hT > self.htCut)\n",
    "        evtweights = evtweights[passhT]\n",
    "        FatJets = FatJets[passhT]\n",
    "        \n",
    "        # ---- FatJets Info ---- #\n",
    "        oneFatJet = (FatJets.counts >= 1) # at least one fat jet\n",
    "        FatJets = FatJets[oneFatJet]\n",
    "        evtweights = evtweights[oneFatJet]\n",
    "        output['cutflow']['one FatJets'] += oneFatJet.sum()\n",
    "        \n",
    "        twoFatJets = (FatJets.counts >= 2)\n",
    "        FatJets = FatJets[twoFatJets]\n",
    "        evtweights = evtweights[twoFatJets]\n",
    "        output['cutflow']['two FatJets'] += twoFatJets.sum()\n",
    "\n",
    "        jet_id = (FatJets.jetId > 0)\n",
    "        \n",
    "        FatJets = FatJets[jet_id]\n",
    "        output['cutflow']['jet id'] += jet_id.any().sum()\n",
    "        \n",
    "        jetkincut_index = (FatJets.pt > self.ak8PtMin) & (np.abs(FatJets.eta) < 2.5) # eta cut here\n",
    "        FatJets = FatJets[ jetkincut_index ]\n",
    "        output['cutflow']['jet kin'] += jetkincut_index.any().sum()\n",
    "\n",
    "        twoFatJetsKin = (FatJets.counts >= 2) # Whats the difference from twoFatJets ?\n",
    "        FatJets = FatJets[twoFatJetsKin]\n",
    "        evtweights = evtweights[twoFatJetsKin]\n",
    "        output['cutflow']['two FatJets and jet kin'] += twoFatJetsKin.sum()\n",
    "        \n",
    "        FatJets = FatJets[:,0:2]\n",
    "        \n",
    "        # ---- Randomly Select AK8 Jet as TTbar Candidate --- #\n",
    "\n",
    "        highPhi = FatJets.phi[:,0] > FatJets.phi[:,1]\n",
    "        highRandIndex = np.where(highPhi, 0, 1)\n",
    "        #index = JaggedArray.fromcounts(np.ones(len(FatJets), dtype='i'), prng.randint(2, size=len(FatJets)))\n",
    "        index = JaggedArray.fromcounts(np.ones(len(FatJets), dtype='i'), highRandIndex )\n",
    "        jet0 = FatJets[index]\n",
    "        jet1 = FatJets[1 - index]\n",
    "\n",
    "        #print(\"index size = \", index.size)\n",
    "        #print(index.tolist())\n",
    "        #print()\n",
    "        \n",
    "        ttbarcands = jet0.cross(jet1) #FatJets[:,0:2].distincts()\n",
    "        \n",
    "        # ---- Look for more than one TTbar candidate and re-broadcast releveant arrays  ---- #\n",
    "        oneTTbar = (ttbarcands.counts >= 1)\n",
    "        output['cutflow']['>= one oneTTbar'] += oneTTbar.sum()\n",
    "        ttbarcands = ttbarcands[oneTTbar]\n",
    "        evtweights = evtweights[oneTTbar]\n",
    "        FatJets = FatJets[oneTTbar]\n",
    "         \n",
    "        # ---- Apply Delta Phi Cut ---- #\n",
    "        dPhiCut = (ttbarcands.i0.p4.delta_phi(ttbarcands.i1.p4) > 2.1).flatten()\n",
    "        output['cutflow']['dPhi > 2.1'] += dPhiCut.sum()\n",
    "        ttbarcands = ttbarcands[dPhiCut]\n",
    "        evtweights = evtweights[dPhiCut]\n",
    "        FatJets = FatJets[dPhiCut] \n",
    "        \n",
    "        \n",
    "        # ---- Get Analysis Categories ---- # \n",
    "        # ---- They are (central, forward)   cross   (0b,1b,>=2b) ---- #\n",
    "        cen = np.abs(ttbarcands.i0.p4.rapidity - ttbarcands.i1.p4.rapidity) < 1.0\n",
    "        fwd = (~cen)\n",
    "        tau32_i0 = np.where(ttbarcands.i0.tau2>0,ttbarcands.i0.tau3/ttbarcands.i0.tau2, 0 )\n",
    "        tau32_i1 = np.where(ttbarcands.i1.tau2>0,ttbarcands.i1.tau3/ttbarcands.i1.tau2, 0 )\n",
    "        taucut_i0 = tau32_i0 < self.tau32Cut\n",
    "        taucut_i1 = tau32_i1 < self.tau32Cut\n",
    "        mcut_i0 = (self.minMSD < ttbarcands.i0.msoftdrop) & (ttbarcands.i0.msoftdrop < self.maxMSD) \n",
    "        mcut_i1 = (self.minMSD < ttbarcands.i1.msoftdrop) & (ttbarcands.i1.msoftdrop < self.maxMSD) \n",
    "\n",
    "        ttag_i0 = (taucut_i0) & (mcut_i0)\n",
    "        ttag_i1 = (taucut_i1) & (mcut_i1)\n",
    "        antitag = (~taucut_i0) & (mcut_i0) #Probe will always be ttbarcands.i1\n",
    "        \n",
    "        ttag0 = (~ttag_i0) & (~ttag_i1)\n",
    "        ttag1 = ttag_i0 ^ ttag_i1\n",
    "        ttag2 = ttag_i0 & ttag_i1\n",
    "\n",
    "        btag_i0 = (ttbarcands.i0.btagDeepB > self.bdisc)\n",
    "        btag_i1 = (ttbarcands.i1.btagDeepB > self.bdisc)\n",
    "        \n",
    "        btag0 = (~btag_i0) & (~btag_i1)\n",
    "        btag1 = btag_i0 ^ btag_i1\n",
    "        btag2 = btag_i0 & btag_i1\n",
    "        \n",
    "        antitag_probe = np.logical_and(antitag, ttag_i1) #Found an antitag and ttagged probe pair for mistag rate\n",
    "        \n",
    "        regs = [cen,fwd]\n",
    "        btags = [btag0,btag1,btag2]\n",
    "        ttags = [antitag_probe,antitag,ttag0,ttag1,ttag2]\n",
    "        cats = [ (t&b&y).flatten() for t,b,y in itertools.product( ttags, btags, regs) ]\n",
    "        labels_and_categories = dict(zip( self.anacats, cats ))\n",
    "        \n",
    "        ttbarmass = ttbarcands.p4.sum().mass.flatten()\n",
    "        jetpt = ttbarcands.pt.flatten()\n",
    "        jeteta = ttbarcands.eta.flatten()\n",
    "        jetphi = ttbarcands.phi.flatten()\n",
    "        jety = ttbarcands.p4.rapidity.flatten()\n",
    "        jetdy = np.abs(ttbarcands.i0.p4.rapidity.flatten() - ttbarcands.i1.p4.rapidity.flatten())\n",
    "        \n",
    "        #print(\"jetpt both = \", ttbarcands.pt.flatten())\n",
    "        #print(\"jetpt control = \", ttbarcands.i0.pt.flatten())\n",
    "        #print(\"jetpt probe = \", ttbarcands.i1.pt.flatten())\n",
    "        #print()\n",
    "        \n",
    "        weights = evtweights.flatten()\n",
    "        #weights[weights < 0] = 0\n",
    "        \n",
    "        # ---- Define the SumW2 for MC Datasets ---- #\n",
    "        output['cutflow']['sumw'] += np.sum(weights)\n",
    "        output['cutflow']['sumw2'] += np.sum(weights**2)\n",
    "        # ---- Define Momentum p of probe jet as the Mistag Rate variable; M(p) ---- #\n",
    "        # ---- Transverse Momentum pT can also be used instead; M(pT) ---- #\n",
    "        pT = ttbarcands.i1.pt.flatten()\n",
    "        eta = ttbarcands.i1.eta.flatten()\n",
    "        pz = np.sinh(eta)*pT\n",
    "        p = np.absolute(np.sqrt(pT**2 + pz**2))\n",
    "        \n",
    "        # ---- Define the Numerator and Denominator for Mistag Rate ---- #\n",
    "        numerator = np.where(antitag_probe, p, -1) # If no antitag and tagged probe, move event to useless bin\n",
    "        denominator = np.where(antitag, p, -1) # If no antitag, move event to useless bin\n",
    "        \n",
    "        df = pd.DataFrame({\"momentum\":p}) # Used for finding values in LookUp Tables\n",
    "        \n",
    "        for ilabel,icat in labels_and_categories.items():\n",
    "            ### ------------------------------------ Mistag Scaling ------------------------------------ ###\n",
    "            if self.UseLookUpTables == True:\n",
    "                file_df = self.lu['at' + str(ilabel[2:])] # get mistag (lookup) filename for 'at'\n",
    "                bin_widths = file_df['p'].values # collect bins as written in .csv file\n",
    "                mtr = file_df['M(p)'].values # collect mistag rate as function of p as written in file\n",
    "                wgts = mtr # Define weights based on mistag rates\n",
    "                \n",
    "                BinKeys = np.arange(bin_widths.size) # Use as label for BinNumber column in the new dataframe\n",
    "                print('\\n bin_widths = \\n', bin_widths)\n",
    "                print('\\n p = \\n', p)\n",
    "                print('\\n BinKeys = \\n', BinKeys)\n",
    "                #Bins = pd.interval_range(start=0, periods=50, freq=140, closed='left') # Recreate the momentum bins from file_df as something readable for pd.cut()\n",
    "                Bins = np.array([400, 500, 600, 800, 1000, 1500, 2000, 3000, 7000, 10000])\n",
    "                print('\\n bin_widths -> Readable Bins = \\n', Bins)\n",
    "                #P = np.where(p>5000., 5000., p) # Avoids momentum larger than the allowed bin size; caps the momentum\n",
    "                df['BinWidth'] = pd.cut(p, bins=Bins) # new dataframe column\n",
    "                df['BinNumber'] = pd.cut(p, bins=Bins, labels=BinKeys)\n",
    "                #df['BinNumber'] = pd.cut(p, bins=Bins).map(dict(zip(Bins,BinKeys))) # new dataframe column\n",
    "                \n",
    "                BinNumber = df['BinNumber'].values # Collect the Bin Numbers into a numpy array\n",
    "                print('\\n Bin Number = \\n', BinNumber)\n",
    "                BinNumber = BinNumber.astype('int64') # Insures the bin numbers are integers\n",
    "            \n",
    "                WeightMatching = wgts[BinNumber] # Match 'wgts' with corresponding p bin using the bin number\n",
    "                Weights = weights*WeightMatching # Include 'wgts' with the previously defined 'weights'\n",
    "            else:\n",
    "                Weights = weights # No mistag rates, no change to weights\n",
    "            ###---------------------------------------------------------------------------------------------###\n",
    "            output['cutflow'][ilabel] += np.sum(icat)\n",
    "          \n",
    "            output['ttbarmass'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                ttbarmass=ttbarmass[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jetpt'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetpt=jetpt[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['probept'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetpt=pT[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['probep'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                Jetp=p[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jeteta'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jeteta=jeteta[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jetphi'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetphi=jetphi[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jety'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jety=jety[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jetdy'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetdy=jetdy[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['numerator'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                Jetp=numerator[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['denominator'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                Jetp=denominator[icat],\n",
    "                                weight=Weights[icat])\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesets = {\n",
    "    #'QCD':qcdfiles,\n",
    "    #'TTbar':ttbarfiles\n",
    "    'JetHT':jetdatafiles\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JetHT\n",
      "['At0bcen', 'At0bfwd', 'At1bcen', 'At1bfwd', 'At2bcen', 'At2bfwd', 'at0bcen', 'at0bfwd', 'at1bcen', 'at1bfwd', 'at2bcen', 'at2bfwd', '0t0bcen', '0t0bfwd', '0t1bcen', '0t1bfwd', '0t2bcen', '0t2bfwd', '1t0bcen', '1t0bfwd', '1t1bcen', '1t1bfwd', '1t2bcen', '1t2bfwd', '2t0bcen', '2t0bfwd', '2t1bcen', '2t1bfwd', '2t2bcen', '2t2bfwd']\n",
      "[####################################### ] | 97% Completed | 36min  5.8s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-844acf18d63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                           \u001b[0;34m'skipbadfiles'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                           'workers': 2},\n\u001b[0;32m---> 25\u001b[0;31m                                       \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                                      )\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mrun_uproot_job\u001b[0;34m(fileset, treename, processor_instance, executor, executor_args, pre_executor, pre_args, chunksize, maxchunks, metadata_cache)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     }\n\u001b[1;32m   1067\u001b[0m     \u001b[0mexe_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m     \u001b[0mexecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexe_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[0mwrapped_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chunks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mdask_executor\u001b[0;34m(items, function, accumulator, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# FIXME: fancy widget doesn't appear, have to live with boring pbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m         \u001b[0mprogress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m     \u001b[0maccumulator\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_maybe_decompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/diagnostics/progressbar.py\u001b[0m in \u001b[0;36mprogress\u001b[0;34m(notebook, multi, complete, *futures, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0mTextProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/diagnostics/progressbar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, keys, scheduler, interval, width, loop, complete, start, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mloop_runner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoopRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mloop_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mrun_sync\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tstart = time.time()\n",
    "\n",
    "outputs_unweighted = {}\n",
    "\n",
    "seed = 1234567890\n",
    "prng = RandomState(seed)\n",
    "\n",
    "for name,files in filesets.items(): \n",
    "    \n",
    "\n",
    "    print(name)\n",
    "    output = processor.run_uproot_job({name:files},\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=TTbarResProcessor(UseLookUpTables=False,\n",
    "                                                                          prng=prng),\n",
    "                                      executor=processor.dask_executor,\n",
    "                                      #executor=processor.iterative_executor,\n",
    "                                      #executor=processor.futures_executor,\n",
    "                                      executor_args={\n",
    "                                          'client': client, \n",
    "                                          'nano':False, \n",
    "                                          'flatten':True, \n",
    "                                          'skipbadfiles':False,\n",
    "                                          'workers': 2},\n",
    "                                      chunksize=100000, maxchunks=100\n",
    "                                     )\n",
    "\n",
    "    elapsed = time.time() - tstart\n",
    "    outputs_unweighted[name] = output\n",
    "    print(output)\n",
    "    util.save(output, 'TTbarResCoffea_' + name + '_unweighted_output.coffea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Elapsed time = ', elapsed, ' sec.')\n",
    "print('Elapsed time = ', elapsed/60., ' min.')\n",
    "print('Elapsed time = ', elapsed/3600., ' hrs.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,output in outputs_unweighted.items(): \n",
    "    print(\"-------Unweighted \" + name + \"--------\")\n",
    "    for i,j in output['cutflow'].items():        \n",
    "        print( '%20s : %12d' % (i,j) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(mypath):\n",
    "    '''Creates a directory. equivalent to using mkdir -p on the command line'''\n",
    "\n",
    "    from errno import EEXIST\n",
    "    from os import makedirs,path\n",
    "\n",
    "    try:\n",
    "        makedirs(mypath)\n",
    "    except OSError as exc: # Python >2.5\n",
    "        if exc.errno == EEXIST and path.isdir(mypath):\n",
    "            pass\n",
    "        else: raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoesDirectoryExist(mypath): #extra precaution (Probably overkill...)\n",
    "    '''Checks to see if Directory exists before running mkdir_p'''\n",
    "    import os.path\n",
    "    from os import path\n",
    "    \n",
    "    if path.exists(mypath):\n",
    "        pass\n",
    "    else:\n",
    "        mkdir_p(mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import re # regular expressions\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- Reiterate categories ---- #\n",
    "ttagcats = [\"at\", \"0t\", \"1t\", \"2t\"]\n",
    "btagcats = [\"0b\", \"1b\", \"2b\"]\n",
    "ycats = ['cen', 'fwd']\n",
    "\n",
    "list_of_cats = [ t+b+y for t,b,y in itertools.product( ttagcats, btagcats, ycats) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maindirectory = os.getcwd() # changes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ---------------- CREATES MISTAG PLOTS ---------------- \"\"\"\n",
    "# ---- Only Use This Cell When LookUp Tables Are Not In Use (i.e. UseLookUpTables = False) ---- #\n",
    "\n",
    "SaveDirectory = maindirectory + '/MistagPlots/'\n",
    "DoesDirectoryExist(SaveDirectory) # no need to create the directory several times\n",
    "\n",
    "# Function x**2\n",
    "def forward(x):\n",
    "    return x**2\n",
    "\n",
    "\n",
    "def inverse(x):\n",
    "    return x**(1/2)\n",
    "\n",
    "print(SaveDirectory)\n",
    "for iset in filesets:\n",
    "    for icat in list_of_cats:\n",
    "        print(iset)\n",
    "        print(icat)\n",
    "        title = iset + ' mistag ' + icat\n",
    "        filename = 'mistag_' + iset + '_' + icat + '.' + 'png'\n",
    "        print(outputs_unweighted[iset]['numerator'])\n",
    "        Numerator = outputs_unweighted[iset]['numerator'].integrate('anacat', icat).integrate('dataset', iset)\n",
    "        Denominator = outputs_unweighted[iset]['denominator'].integrate('anacat', icat).integrate('dataset', iset)\n",
    "        print(Numerator)\n",
    "        print(Denominator)\n",
    "        mistag = hist.plotratio(num = Numerator, denom = Denominator,\n",
    "                                error_opts={'marker': '.', 'markersize': 10., 'color': 'k', 'elinewidth': 1},\n",
    "                                unc = 'num')\n",
    "        plt.title(title)\n",
    "        plt.ylim(bottom = 0, top = 0.1)\n",
    "        plt.xlim(left = 100, right = 10000)\n",
    "        #plt.xticks(np.array([0, 500, 600, 700]))\n",
    "        #mistag.set_xscale('function', functions=(forward, inverse))\n",
    "        mistag.set_xscale('log')\n",
    "        \n",
    "        #plt.savefig(SaveDirectory+filename, bbox_inches=\"tight\")\n",
    "        #print(filename + ' saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ---------------- LOOK UP TABLE ---------------- \"\"\"\n",
    "# ---- Only Use This Cell When LookUp Tables Are Not In Use (i.e. UseLookUpTables = False) ---- #\n",
    "\n",
    "\n",
    "runLUTS = True \n",
    "\n",
    "luts = {}\n",
    "\n",
    "if runLUTS : \n",
    "\n",
    "    SaveDirectory = maindirectory + '/LookupTables/'\n",
    "    DoesDirectoryExist(SaveDirectory)\n",
    "\n",
    "\n",
    "\n",
    "    for iset in filesets:\n",
    "        print('\\t\\tfileset: ' + iset + '\\n*****************************************************\\n')\n",
    "        for icat in list_of_cats:\n",
    "            title = iset + ' mistag ' + icat\n",
    "            filename = 'mistag_' + iset + '_' + icat + '.' + 'csv'\n",
    "\n",
    "            Numerator = outputs_unweighted[iset]['numerator'].integrate('anacat',icat).integrate('dataset',iset)\n",
    "            Denominator = outputs_unweighted[iset]['denominator'].integrate('anacat',icat).integrate('dataset',iset)\n",
    "            \n",
    "            \n",
    "            \n",
    "            N_vals = Numerator.values()[()]\n",
    "            D_vals = Denominator.values()[()]\n",
    "            print(N_vals)\n",
    "            print(D_vals)\n",
    "            print()\n",
    "            mistag_vals = np.where(D_vals > 0, N_vals / D_vals, 0)\n",
    "\n",
    "\n",
    "            p_vals = [] # Momentum values\n",
    "            for iden in Numerator.identifiers('Jetp'):\n",
    "                p_vals.append(iden)\n",
    "                \n",
    "                \n",
    "\n",
    "            print('fileset:  ' + iset)\n",
    "            print('category: ' + icat)\n",
    "            print('________________________________________________\\n')\n",
    "\n",
    "            d = {'p': p_vals, 'M(p)': mistag_vals}\n",
    "            \n",
    "            #print(\"p vals = \", p_vals)\n",
    "            print(\"d vals = \", d)\n",
    "            print()\n",
    "            df = pd.DataFrame(data=d)\n",
    "            luts[icat] = df\n",
    "\n",
    "            with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
    "                print(df)\n",
    "            print('\\n')\n",
    "\n",
    "            df.to_csv(SaveDirectory+filename) # use later to collect bins and weights for re-scaling\n",
    "else :\n",
    "    for iset in filesets:\n",
    "        print('\\t\\tfileset: ' + iset + '\\n*****************************************************\\n')\n",
    "        for icat in list_of_cats:\n",
    "            title = iset + ' mistag ' + icat\n",
    "            filename = 'mistag_' + iset + '_' + icat + '.' + 'csv'\n",
    "            luts[icat] = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Events/s:\", output['cutflow']['all events']/elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tstart = time.time()\n",
    "\n",
    "fileset = {\n",
    "    'QCD':qcdfiles\n",
    "}\n",
    "\n",
    "outputs_weighted = {}\n",
    "\n",
    "prng = RandomState(seed)\n",
    "\n",
    "output_weighted = processor.run_uproot_job(fileset,\n",
    "                                  treename='Events',\n",
    "                                  processor_instance=TTbarResProcessor(UseLookUpTables=True, lu=luts,\n",
    "                                                                      prng=prng),\n",
    "                                  executor=processor.dask_executor,\n",
    "                                  #executor=processor.iterative_executor,\n",
    "                                  #executor=processor.futures_executor,\n",
    "                                  executor_args={\n",
    "                                      'client': client, \n",
    "                                      'nano':False, \n",
    "                                      'flatten':True, \n",
    "                                      'skipbadfiles':False,\n",
    "                                      'workers': 2},\n",
    "                                  chunksize=100000#, maxchunks=10\n",
    "                                 )\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "outputs_weighted['QCD'] = output_weighted\n",
    "print(output_weighted)\n",
    "print(elapsed)\n",
    "util.save(output_weighted, 'TTbarResCoffea_full_weighted_QCD_output.coffea')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesets = {\n",
    "    #'QCD':qcdfiles,\n",
    "    #'TTbar':ttbarfiles\n",
    "    'JetHT':jetdatafiles\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart = time.time()\n",
    "\n",
    "outputs_weighted = {}\n",
    "\n",
    "prng = RandomState(seed)\n",
    "\n",
    "for name,files in filesets.items(): \n",
    "    \n",
    "\n",
    "    print(name)\n",
    "    output = processor.run_uproot_job({name:files},\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=TTbarResProcessor(UseLookUpTables=True,\n",
    "                                                                           lu=luts,\n",
    "                                                                           prng=prng),\n",
    "                                      executor=processor.dask_executor,\n",
    "                                      #executor=processor.iterative_executor,\n",
    "                                      #executor=processor.futures_executor,\n",
    "                                      executor_args={\n",
    "                                          'client': client, \n",
    "                                          'nano':False, \n",
    "                                          'flatten':True, \n",
    "                                          'skipbadfiles':False,\n",
    "                                          'workers': 2},\n",
    "                                      chunksize=100000, maxchunks=100\n",
    "                                     )\n",
    "\n",
    "    elapsed = time.time() - tstart\n",
    "    outputs_weighted[name] = output\n",
    "    print(output)\n",
    "    util.save(output, 'TTbarResCoffea_' + name + '_weighted_output.coffea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name,output in outputs_weighted.items(): \n",
    "    print(\"-------Unweighted \" + name + \"--------\")\n",
    "    for i,j in output['cutflow'].items():        \n",
    "        print( '%20s : %12d' % (i,j) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
