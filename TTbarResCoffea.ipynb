{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TTbarResCoffea` Notebook to perform the data-driven mistag-rate-based ttbar hadronic analysis. \n",
    "This module must be run twice: \n",
    "   1. Make the mistag rate in the \"anti-tag and probe\" selection \n",
    "and the expectation in the signal region from MC,\n",
    "   1. Applies that mistag rate and the mod-mass procedure to the single-tag selection. \n",
    "\n",
    "These are all done in bins of\n",
    "b-tag categories (0, 1, $\\ge 2$) and rapidity ($|y| \\le 1.0$, $|y| > 1.0$).\n",
    "The signal region is two top-tagged jets. \n",
    "The background estimate is the single-tag selection weighted by the mistag rate from the\n",
    "\"anti-tag and probe\" region, with the mass of the weighted jet set to a random\n",
    "value from QCD MC in the 1-ttag region. \n",
    "\n",
    "\n",
    "The preselection is:\n",
    "- AK4-based $H_{T} > 1100$ GeV (to be on the trigger plateau). \n",
    "- $\\ge 2$ AK8 jets with AK8 $p_{T} > 400$ GeV and $|y| < 2.5$, loose jet ID applied from matched AK4 jets\n",
    "\n",
    "The 1-tag selection adds:\n",
    "- $\\ge 1$ AK8 jet with top tagging applied to randomly-assigned tag jet. \n",
    "\n",
    "\n",
    "The anti-tag selection is disjoint from the 1-tag selection:\n",
    "- $\\ge 1$ AK8 jet with top tagging VETO applied to randomly-assigned tag jet. \n",
    "\n",
    "\n",
    "The 2-tag selection is:\n",
    "- $\\ge 2$ AK8 jets with top tagging applied to both leading jets. \n",
    "\n",
    "\n",
    "The ttbar candidate mass assumes the two leading top-tagged jets are the top quarks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n/JetHT/Run2016B_ver1-Nano25Oct2019_ver1-v1/NANOAOD\\n/JetHT/Run2016B_ver2-Nano25Oct2019_ver2-v1/NANOAOD\\n/JetHT/Run2016C-Nano25Oct2019-v1/NANOAOD\\n/JetHT/Run2016D-Nano25Oct2019-v1/NANOAOD\\n/JetHT/Run2016E-Nano25Oct2019-v1/NANOAOD\\n/JetHT/Run2016F-Nano25Oct2019-v1/NANOAOD\\n/JetHT/Run2016G-Nano25Oct2019-v1/NANOAOD\\n/JetHT/Run2016H-Nano25Oct2019-v1/NANOAOD\\n/JetHT/Run2017B-Nano25Oct2019-v1/NANOAOD\\n/JetHT/Run2017C-Nano25Oct2019-v1/NANOAOD\\n/JetHT/Run2017D-Nano25Oct2019-v1/NANOAOD\\n/JetHT/Run2017E-Nano25Oct2019-v1/NANOAOD\\n/JetHT/Run2017F-Nano25Oct2019-v1/NANOAOD\\n/JetHT/Run2018A-Nano25Oct2019-v1/NANOAOD\\n/JetHT/Run2018B-Nano25Oct2019-v1/NANOAOD\\n/JetHT/Run2018C-Nano25Oct2019-v2/NANOAOD\\n/JetHT/Run2018D-Nano25Oct2019_ver2-v1/NANOAOD\\n\\n/TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_102X_upgrade2018_realistic_v21_ext1-v1/NANOAODSIM\\n/QCD_Pt-15to7000_TuneCP5_Flat_13TeV_pythia8/RunIIFall17NanoAODv7-PU2017_12Apr2018_Nano02Apr2020_102X_mc2017_realistic_v8-v1/NANOAODSIM\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- All Datasets used for analysis and dasgoclient searches ---- #\n",
    "\n",
    "\"\"\"\n",
    "/JetHT/Run2016B_ver1-Nano25Oct2019_ver1-v1/NANOAOD\n",
    "/JetHT/Run2016B_ver2-Nano25Oct2019_ver2-v1/NANOAOD\n",
    "/JetHT/Run2016C-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2016D-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2016E-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2016F-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2016G-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2016H-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2017B-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2017C-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2017D-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2017E-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2017F-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2018A-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2018B-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2018C-Nano25Oct2019-v2/NANOAOD\n",
    "/JetHT/Run2018D-Nano25Oct2019_ver2-v1/NANOAOD\n",
    "\n",
    "/TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_102X_upgrade2018_realistic_v21_ext1-v1/NANOAODSIM\n",
    "/QCD_Pt-15to7000_TuneCP5_Flat_13TeV_pythia8/RunIIFall17NanoAODv7-PU2017_12Apr2018_Nano02Apr2020_102X_mc2017_realistic_v8-v1/NANOAODSIM\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor\n",
    "from coffea import util\n",
    "from awkward import JaggedArray\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrootdstr1 = 'root://cmseos.fnal.gov//'\n",
    "xrootdstr2 = 'root://cmsxrootd.fnal.gov//'\n",
    "xrootdstr3 = 'root://cmsxrootd-site.fnal.gov/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcdfilename = 'QCD.txt'\n",
    "with open(qcdfilename) as f:\n",
    "    qcdfiles = [xrootdstr2 + s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttbarfilename = 'TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8.txt'\n",
    "with open(ttbarfilename) as f:\n",
    "    ttbarfiles = [xrootdstr2 + s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetdatafilename = 'JetHT_Data.txt'\n",
    "with open(jetdatafilename) as f:\n",
    "    jetdatafiles = [xrootdstr2 + s.strip() for s in f.readlines()[::3]] # Every third datafile\n",
    "with open(jetdatafilename) as g:\n",
    "    jetdatafiles2016 = [xrootdstr2 + s.strip() for s in g.readlines() if \"/store/data/Run2016\" in s]#[0::2]\n",
    "with open(jetdatafilename) as h:\n",
    "    jetdatafiles2017 = [xrootdstr2 + s.strip() for s in h.readlines() if \"/store/data/Run2017\" in s]\n",
    "with open(jetdatafilename) as i:\n",
    "    jetdatafiles2018 = [xrootdstr2 + s.strip() for s in i.readlines() if \"/store/data/Run2018\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(jetdatafiles[2]) # Test to see if correct files are collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from columnservice.client import ColumnClient\n",
    "cc = ColumnClient(\"coffea-dask.fnal.gov\")\n",
    "client = cc.get_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400, 500, 600, 700, 800, 900, 1000, 1200, 1500, 2000, 3000, 7000]\n"
     ]
    }
   ],
   "source": [
    "btag0_bins = []\n",
    "ix = 400\n",
    "while ix <= 7000:\n",
    "    btag0_bins.append(ix)\n",
    "    if ix < 1000:\n",
    "        ix += 100\n",
    "    elif ix < 1200:\n",
    "        ix += 200\n",
    "    elif ix < 1500:\n",
    "        ix += 300\n",
    "    elif ix < 2000:\n",
    "        ix += 500\n",
    "    elif ix < 3000:\n",
    "        ix += 1000\n",
    "    else:\n",
    "        ix += 4000\n",
    "print(btag0_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_bins = [400, 500, 600, 800, 1000, 1500, 2000, 3000, 7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"@TTbarResAnaHadronic Package to perform the data-driven mistag-rate-based ttbar hadronic analysis. \n",
    "\"\"\"\n",
    "class TTbarResProcessor(processor.ProcessorABC):\n",
    "    def __init__(self, prng, htCut=950., minMSD=105., maxMSD=210., tau32Cut=0.65, ak8PtMin=400., bdisc=0.7,\n",
    "                writePredDist=True,isData=True,year=2019, UseLookUpTables=False,\n",
    "                lu = None):\n",
    "        \n",
    "        self.prng = prng\n",
    "        self.htCut = htCut\n",
    "        self.minMSD = minMSD\n",
    "        self.maxMSD = maxMSD\n",
    "        self.tau32Cut = tau32Cut\n",
    "        self.ak8PtMin = ak8PtMin\n",
    "        self.bdisc = bdisc\n",
    "        self.writePredDist = writePredDist\n",
    "        self.writeHistFile = True\n",
    "        self.isData = isData\n",
    "        self.year=year\n",
    "        self.UseLookUpTables = UseLookUpTables\n",
    "        self.lu = lu # Look Up Tables\n",
    "        \n",
    "        self.ttagcats = [\"At\",\"at\", \"0t\", \"1t\", \"2t\"] #anti-tag+probe, anti-tag, 0, 1, 2 ttags\n",
    "        self.btagcats = [\"0b\", \"1b\", \"2b\"]   # 0, 1, >=2 btags\n",
    "        self.ycats = ['cen', 'fwd']          # Central and forward\n",
    "        # Combine categories like \"0bcen\", \"0bfwd\", etc:\n",
    "        self.anacats = [ t+b+y for t,b,y in itertools.product( self.ttagcats, self.btagcats, self.ycats) ]\n",
    "        print(self.anacats)\n",
    "        \n",
    "        dataset_axis = hist.Cat(\"dataset\", \"Primary dataset\")\n",
    "        cats_axis = hist.Cat(\"anacat\", \"Analysis Category\")\n",
    "        \n",
    "        jetmass_axis = hist.Bin(\"jetmass\", r\"Jet $m$ [GeV]\", 50, 0, 500)\n",
    "        jetpt_axis = hist.Bin(\"jetpt\", r\"Jet $p_{T}$ [GeV]\", 50, 0, 5000)\n",
    "        ttbarmass_axis = hist.Bin(\"ttbarmass\", r\"$m_{t\\bar{t}}$ [GeV]\", 50, 0, 5000)\n",
    "        jeteta_axis = hist.Bin(\"jeteta\", r\"Jet $\\eta$\", 50, -5, 5)\n",
    "        jetphi_axis = hist.Bin(\"jetphi\", r\"Jet $\\phi$\", 50, -np.pi, np.pi)\n",
    "        jety_axis = hist.Bin(\"jety\", r\"Jet $y$\", 50, -3, 3)\n",
    "        jetdy_axis = hist.Bin(\"jetdy\", r\"Jet $\\Delta y$\", 50, 0, 5)\n",
    "        #jetp_axis = hist.Bin(\"jetp\", r\"Jet $p$ [GeV]\", 100, 0, 10000)\n",
    "        manual_axis = hist.Bin(\"jetp\", r\"Jet Momentum [GeV]\", manual_bins)\n",
    "        tagger_axis = hist.Bin(\"tagger\", r\"deepTag\", 50, 0, 1)\n",
    "        \n",
    "        subjetmass_axis = hist.Bin(\"subjetmass\", r\"SubJet $m$ [GeV]\", 50, 0, 500)\n",
    "        subjetpt_axis = hist.Bin(\"subjetpt\", r\"SubJet $p_{T}$ [GeV]\", 50, 0, 2000)\n",
    "        subjeteta_axis = hist.Bin(\"subjeteta\", r\"SubJet $\\eta$\", 50, -4, 4)\n",
    "        subjetphi_axis = hist.Bin(\"subjetphi\", r\"SubJet $\\phi$\", 50, -np.pi, np.pi)\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'ttbarmass': hist.Hist(\"Counts\", dataset_axis, cats_axis, ttbarmass_axis),\n",
    "            \n",
    "            'jetmass':   hist.Hist(\"Counts\", dataset_axis, cats_axis, jetmass_axis),\n",
    "            'jetpt':     hist.Hist(\"Counts\", dataset_axis, cats_axis, jetpt_axis),\n",
    "            'jeteta':    hist.Hist(\"Counts\", dataset_axis, cats_axis, jeteta_axis),\n",
    "            'jetphi':    hist.Hist(\"Counts\", dataset_axis, cats_axis, jetphi_axis),\n",
    "            \n",
    "            'probept':   hist.Hist(\"Counts\", dataset_axis, cats_axis, jetpt_axis),\n",
    "            'probep':    hist.Hist(\"Counts\", dataset_axis, cats_axis, manual_axis),\n",
    "            \n",
    "            'jety':      hist.Hist(\"Counts\", dataset_axis, cats_axis, jety_axis),\n",
    "            'jetdy':     hist.Hist(\"Counts\", dataset_axis, cats_axis, jetdy_axis),\n",
    "            \n",
    "            'deepTag_TvsQCD':   hist.Hist(\"Counts\", dataset_axis, cats_axis, jetpt_axis, tagger_axis),\n",
    "            'deepTagMD_TvsQCD': hist.Hist(\"Counts\", dataset_axis, cats_axis, jetpt_axis, tagger_axis),\n",
    "            \n",
    "            'subjetmass':   hist.Hist(\"Counts\", dataset_axis, cats_axis, subjetmass_axis),\n",
    "            'subjetpt':     hist.Hist(\"Counts\", dataset_axis, cats_axis, subjetpt_axis),\n",
    "            'subjeteta':    hist.Hist(\"Counts\", dataset_axis, cats_axis, subjeteta_axis),\n",
    "            'subjetphi':    hist.Hist(\"Counts\", dataset_axis, cats_axis, subjetphi_axis),\n",
    "            \n",
    "            'numerator':   hist.Hist(\"Counts\", dataset_axis, cats_axis, manual_axis),\n",
    "            'denominator': hist.Hist(\"Counts\", dataset_axis, cats_axis, manual_axis),\n",
    "            \n",
    "            'cutflow': processor.defaultdict_accumulator(int),\n",
    "            \n",
    "        })\n",
    "\n",
    "            \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, df):\n",
    "        \n",
    "        output = self.accumulator.identity()\n",
    "        #dataset = events.metadata['dataset']\n",
    "        \n",
    "        # ---- Define dataset ---- #\n",
    "        dataset = df['dataset'] #coffea.processor.LazyDataFrame\n",
    "        Dataset_info = df.available #list of available columns in LazyDataFrame object (Similar to 'Events->Show()' command in ROOT)\n",
    "        \n",
    "        # ---- Get triggers from Dataset_info ---- #\n",
    "        triggers = [itrig for itrig in Dataset_info if 'HLT_PFHT' in itrig]\n",
    "        AK8triggers = [itrig for itrig in Dataset_info if 'HLT_AK8PFHT' in itrig]\n",
    "\n",
    "        # ---- Find numeric values in trigger strings ---- #\n",
    "        triggers_cut1 = [sub.split('PFHT')[1] for sub in triggers] # Remove string characters from left of number\n",
    "        triggers_cut2 = [sub.split('_')[0] for sub in triggers_cut1] # Remove string characters from right of number\n",
    "        isTriggerValue = [val.isnumeric() for val in triggers_cut2] # Boolean -> if string is only a number\n",
    "        triggers_cut2 = np.where(isTriggerValue, triggers_cut2, 0) # If string is not a number, replace with 0\n",
    "        triggers_vals = [int(val) for val in triggers_cut2] # Convert string numbers to integers\n",
    "        \n",
    "        AK8triggers_cut1 = [sub.split('HT')[1] for sub in AK8triggers]\n",
    "        AK8triggers_cut2 = [sub.split('_')[0] for sub in AK8triggers_cut1]\n",
    "        isAK8TriggerValue = [val.isnumeric() for val in AK8triggers_cut2]\n",
    "        AK8triggers_cut2 = np.where(isAK8TriggerValue, AK8triggers_cut2, 0)\n",
    "        AK8triggers_vals = [int(val) for val in AK8triggers_cut2]\n",
    "        \n",
    "        # ---- Find Largest and Second Largest Value ---- #\n",
    "        triggers_vals.sort(reverse = True)\n",
    "        AK8triggers_vals.sort(reverse = True)\n",
    "        \n",
    "        triggers_vals1 = str(triggers_vals[0])\n",
    "        triggers_vals2 = str(triggers_vals[1])\n",
    "        AK8triggers_vals1 = str(AK8triggers_vals[0])\n",
    "        AK8triggers_vals2 = str(AK8triggers_vals[1])\n",
    "        \n",
    "        # ---- Define strings for the selected triggers ---- #\n",
    "        HLT_trig1_str = [itrig for itrig in triggers if (triggers_vals1) in itrig][0]\n",
    "        HLT_trig2_str = [itrig for itrig in triggers if (triggers_vals2) in itrig][0]\n",
    "        HLT_AK8_trig1_str = [itrig for itrig in AK8triggers if (AK8triggers_vals1) in itrig][0]\n",
    "        HLT_AK8_trig2_str = [itrig for itrig in AK8triggers if (AK8triggers_vals2) in itrig][0]\n",
    "        \n",
    "        # ---- Define HLT triggers to be used ---- #\n",
    "        HLT_trig1 = df[HLT_trig1_str]\n",
    "        HLT_trig2 = df[HLT_trig2_str]\n",
    "        HLT_AK8_trig1 = df[HLT_AK8_trig1_str]\n",
    "        HLT_AK8_trig2 = df[HLT_AK8_trig2_str]\n",
    "       \n",
    "        \n",
    "        # ---- Define AK8 Jets as FatJets ---- #\n",
    "        FatJets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nFatJet'],\n",
    "            pt=df['FatJet_pt'],\n",
    "            eta=df['FatJet_eta'],\n",
    "            phi=df['FatJet_phi'],\n",
    "            mass=df['FatJet_mass'],\n",
    "            area=df['FatJet_area'],\n",
    "            msoftdrop=df['FatJet_msoftdrop'],\n",
    "            jetId=df['FatJet_jetId'],\n",
    "            tau1=df['FatJet_tau1'],\n",
    "            tau2=df['FatJet_tau2'],\n",
    "            tau3=df['FatJet_tau3'],\n",
    "            tau4=df['FatJet_tau4'],\n",
    "            n3b1=df['FatJet_n3b1'],\n",
    "            btagDeepB=df['FatJet_btagDeepB'],\n",
    "            btagCSVV2=df['FatJet_btagCSVV2'],\n",
    "            deepTag_TvsQCD=df['FatJet_deepTag_TvsQCD'],\n",
    "            deepTagMD_TvsQCD=df['FatJet_deepTagMD_TvsQCD'],\n",
    "            subJetIdx1=df['FatJet_subJetIdx1'],\n",
    "            subJetIdx2=df['FatJet_subJetIdx2']\n",
    "            )\n",
    "        \n",
    "        # ---- Define AK4 jets as Jets ---- #\n",
    "        Jets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nJet'],\n",
    "            pt=df['Jet_pt'],\n",
    "            eta=df['Jet_eta'],\n",
    "            phi=df['Jet_phi'],\n",
    "            mass=df['Jet_mass'],\n",
    "            area=df['Jet_area']\n",
    "            )\n",
    "        # ---- Define SubJets ---- #\n",
    "        SubJets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nSubJet'],\n",
    "            pt=df['SubJet_pt'],\n",
    "            eta=df['SubJet_eta'],\n",
    "            phi=df['SubJet_phi'],\n",
    "            mass=df['SubJet_mass'],\n",
    "            btagDeepB=df['SubJet_btagDeepB'],\n",
    "            btagCSVV2=df['SubJet_btagCSVV2']\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # ---- Get event weights from dataset ---- #\n",
    "        if 'JetHT' in dataset: # If data is used...\n",
    "            evtweights = np.ones(FatJets.size) # set all \"data weights\" to one\n",
    "        else: # if Monte Carlo dataset is used...\n",
    "            evtweights = df[\"Generator_weight\"].reshape(-1, 1).flatten()\n",
    "        \n",
    "        # ---- Show all events ---- #\n",
    "        output['cutflow']['all events'] += FatJets.size\n",
    "        \n",
    "        # ---- Apply Trigger(s) ---- #\n",
    "        #FatJets = FatJets[HLT_AK8_trig1]\n",
    "        #evtweights = evtweights[HLT_AK8_trig1]\n",
    "        #Jets = Jets[HLT_AK8_trig1]\n",
    "        #SubJets = SubJets[HLT_AK8_trig1]\n",
    "    \n",
    "        # ---- Jets that satisfy Jet ID ---- #\n",
    "        jet_id = (FatJets.jetId > 0)\n",
    "        FatJets = FatJets[jet_id]\n",
    "        output['cutflow']['jet id'] += jet_id.any().sum()\n",
    "        \n",
    "        # ---- Apply Eta cut ---- #\n",
    "        jetkincut_index = (FatJets.pt > self.ak8PtMin) & (np.abs(FatJets.eta) < 2.5) # eta cut here\n",
    "        FatJets = FatJets[ jetkincut_index ]\n",
    "        output['cutflow']['jet kin'] += jetkincut_index.any().sum()\n",
    "        \n",
    "        # ---- Apply Rapidity cut ---- #\n",
    "        #jetkincut_index = (FatJets.pt > self.ak8PtMin) & (np.abs(FatJets.p4.rapidity) < 2.5) # y cut here\n",
    "        #FatJets = FatJets[ jetkincut_index ]\n",
    "        #output['cutflow']['jet kin'] += jetkincut_index.any().sum()\n",
    "        \n",
    "        # ---- Ensure that FatJets are AK8 Jets ---- #\n",
    "        #ak8Jets = FatJets.area > np.pi*0.8**2\n",
    "        #FatJets = FatJets[ak8Jets]\n",
    "        \n",
    "        # ---- Find at least two AK8 Jets ---- #\n",
    "        twoFatJetsKin = (FatJets.counts >= 2)\n",
    "        FatJets = FatJets[twoFatJetsKin]\n",
    "        evtweights = evtweights[twoFatJetsKin]\n",
    "        Jets = Jets[twoFatJetsKin]\n",
    "        SubJets = SubJets[twoFatJetsKin]\n",
    "        output['cutflow']['two FatJets and jet kin'] += twoFatJetsKin.sum()\n",
    "        \n",
    "        # ---- Apply HT Cut ---- #\n",
    "        #ak4Jets = Jets.area > np.pi*0.4**2\n",
    "        #Jets = Jets[ak4Jets]\n",
    "        hT = Jets.pt.sum()\n",
    "        passhT = (hT > self.htCut)\n",
    "        evtweights = evtweights[passhT]\n",
    "        FatJets = FatJets[passhT]\n",
    "        SubJets = SubJets[passhT]\n",
    "        \n",
    "        # ---- Randomly Select AK8 Jet as TTbar Candidate --- #\n",
    "        highPhi = FatJets.phi[:,0] > FatJets.phi[:,1]\n",
    "        highRandIndex = np.where(highPhi, 0, 1)\n",
    "        #index = JaggedArray.fromcounts(np.ones(len(FatJets), dtype='i'), prng.randint(2, size=len(FatJets)))\n",
    "        index = JaggedArray.fromcounts(np.ones(len(FatJets), dtype='i'), highRandIndex )\n",
    "        jet0 = FatJets[index]\n",
    "        jet1 = FatJets[1 - index]\n",
    "        \n",
    "        ttbarcands = jet0.cross(jet1) #FatJets[:,0:2].distincts()\n",
    "        \n",
    "        # ---- Look for at least 1 TTbar candidate pair and re-broadcast releveant arrays  ---- #\n",
    "        oneTTbar = (ttbarcands.counts >= 1)\n",
    "        output['cutflow']['>= one oneTTbar'] += oneTTbar.sum()\n",
    "        ttbarcands = ttbarcands[oneTTbar]\n",
    "        evtweights = evtweights[oneTTbar]\n",
    "        FatJets = FatJets[oneTTbar]\n",
    "        SubJets = SubJets[oneTTbar]\n",
    "         \n",
    "        # ---- Apply Delta Phi Cut for Back to Back Topology ---- #\n",
    "        dPhiCut = (ttbarcands.i0.p4.delta_phi(ttbarcands.i1.p4) > 2.1).flatten()\n",
    "        output['cutflow']['dPhi > 2.1'] += dPhiCut.sum()\n",
    "        ttbarcands = ttbarcands[dPhiCut]\n",
    "        evtweights = evtweights[dPhiCut]\n",
    "        FatJets = FatJets[dPhiCut] \n",
    "        SubJets = SubJets[dPhiCut] \n",
    "        \n",
    "        # ---- Identify subjets according to subjet ID ---- #\n",
    "        #btag_i0 = (ttbarcands.i0.btagCSVV2 > self.bdisc)\n",
    "        #btag_i1 = (ttbarcands.i1.btagCSVV2 > self.bdisc)\n",
    "    \n",
    "        hasSubjets0 = ((ttbarcands.i0.subJetIdx1 > -1) & (ttbarcands.i0.subJetIdx2 > -1))\n",
    "        hasSubjets1 = ((ttbarcands.i1.subJetIdx1 > -1) & (ttbarcands.i1.subJetIdx2 > -1))\n",
    "        GoodSubjets = ((hasSubjets0) & (hasSubjets1)).flatten()\n",
    "   \n",
    "        ttbarcands = ttbarcands[GoodSubjets] \n",
    "        SubJets = SubJets[GoodSubjets]\n",
    "        evtweights = evtweights[GoodSubjets]\n",
    "       \n",
    "        SubJet01 = SubJets[ttbarcands.i0.subJetIdx1] # FatJet i0 with subjet 1\n",
    "        SubJet02 = SubJets[ttbarcands.i0.subJetIdx2] # FatJet i0 with subjet 2\n",
    "        SubJet11 = SubJets[ttbarcands.i1.subJetIdx1] # FatJet i1 with subjet 1\n",
    "        SubJet12 = SubJets[ttbarcands.i1.subJetIdx2] # FatJet i1 with subjet 2\n",
    "        \n",
    "        # ---- Get Analysis Categories ---- # \n",
    "        # ---- They are (central, forward)   cross   (0b,1b,>=2b) ---- #\n",
    "        cen = np.abs(ttbarcands.i0.p4.rapidity - ttbarcands.i1.p4.rapidity) < 1.0\n",
    "        fwd = (~cen)\n",
    "        tau32_i0 = np.where(ttbarcands.i0.tau2>0,ttbarcands.i0.tau3/ttbarcands.i0.tau2, 0 )\n",
    "        tau32_i1 = np.where(ttbarcands.i1.tau2>0,ttbarcands.i1.tau3/ttbarcands.i1.tau2, 0 )\n",
    "        taucut_i0 = tau32_i0 < self.tau32Cut\n",
    "        taucut_i1 = tau32_i1 < self.tau32Cut\n",
    "        mcut_i0 = (self.minMSD < ttbarcands.i0.msoftdrop) & (ttbarcands.i0.msoftdrop < self.maxMSD) \n",
    "        mcut_i1 = (self.minMSD < ttbarcands.i1.msoftdrop) & (ttbarcands.i1.msoftdrop < self.maxMSD) \n",
    "\n",
    "        ttag_i0 = (taucut_i0) & (mcut_i0)\n",
    "        ttag_i1 = (taucut_i1) & (mcut_i1)\n",
    "        antitag = (~taucut_i0) & (mcut_i0) #Probe will always be ttbarcands.i1\n",
    "        \n",
    "        ttag0 = (~ttag_i0) & (~ttag_i1)\n",
    "        ttag1 = ttag_i0 ^ ttag_i1\n",
    "        ttag2 = ttag_i0 & ttag_i1\n",
    "        \n",
    "        # ---- Pick FatJet that passes btag cut based on its subjet with the highest btag value ---- # \n",
    "        btag_i0 = ( np.maximum(SubJet01.btagCSVV2 , SubJet02.btagCSVV2) > self.bdisc )\n",
    "        btag_i1 = ( np.maximum(SubJet11.btagCSVV2 , SubJet12.btagCSVV2) > self.bdisc )\n",
    "\n",
    "        btag0 = (~btag_i0) & (~btag_i1)\n",
    "        btag1 = btag_i0 ^ btag_i1\n",
    "        btag2 = btag_i0 & btag_i1\n",
    "        \n",
    "        antitag_probe = np.logical_and(antitag, ttag_i1) #Found an antitag and ttagged probe pair for mistag rate\n",
    "        \n",
    "        regs = [cen,fwd]\n",
    "        btags = [btag0,btag1,btag2]\n",
    "        ttags = [antitag_probe,antitag,ttag0,ttag1,ttag2]\n",
    "        cats = [ (t&b&y).flatten() for t,b,y in itertools.product( ttags, btags, regs) ]\n",
    "        labels_and_categories = dict(zip( self.anacats, cats ))\n",
    "        \n",
    "        ttbarmass = ttbarcands.p4.sum().mass.flatten()\n",
    "        jetpt = ttbarcands.pt.flatten()\n",
    "        jeteta = ttbarcands.eta.flatten()\n",
    "        jetphi = ttbarcands.phi.flatten()\n",
    "        jety = ttbarcands.p4.rapidity.flatten()\n",
    "        jetdy = np.abs(ttbarcands.i0.p4.rapidity.flatten() - ttbarcands.i1.p4.rapidity.flatten())\n",
    "        \n",
    "        deepTag = ttbarcands.i1.deepTag_TvsQCD.flatten()\n",
    "        deepTagMD = ttbarcands.i1.deepTagMD_TvsQCD.flatten()\n",
    "      \n",
    "        weights = evtweights.flatten()\n",
    "        #weights[weights < 0] = 0\n",
    "        \n",
    "        # ---- Define the SumW2 for MC Datasets ---- #\n",
    "        output['cutflow']['sumw'] += np.sum(weights)\n",
    "        output['cutflow']['sumw2'] += np.sum(weights**2)\n",
    "        \n",
    "        # ---- Define Momentum p of probe jet as the Mistag Rate variable; M(p) ---- #\n",
    "        # ---- Transverse Momentum pT can also be used instead; M(pT) ---- #\n",
    "        pT = ttbarcands.i1.pt.flatten()\n",
    "        eta = ttbarcands.i1.eta.flatten()\n",
    "        pz = np.sinh(eta)*pT\n",
    "        p = np.absolute(np.sqrt(pT**2 + pz**2))\n",
    "        \n",
    "        # ---- Define the Numerator and Denominator for Mistag Rate ---- #\n",
    "        numerator = np.where(antitag_probe, p, -1) # If no antitag and tagged probe, move event to useless bin\n",
    "        denominator = np.where(antitag, p, -1) # If no antitag, move event to useless bin\n",
    "        \n",
    "        df = pd.DataFrame({\"momentum\":p}) # Used for finding values in LookUp Tables\n",
    "        \n",
    "        for ilabel,icat in labels_and_categories.items():\n",
    "            ### ------------------------------------ Mistag Scaling ------------------------------------ ###\n",
    "            if self.UseLookUpTables == True:\n",
    "                file_df = self.lu[dataset]['at' + str(ilabel[2:])] # get mistag (lookup) filename for 'at'\n",
    "                bin_widths = file_df['p'].values # collect bins as written in .csv file\n",
    "                mtr = file_df['M(p)'].values # collect mistag rate as function of p as written in file\n",
    "                wgts = mtr # Define weights based on mistag rates\n",
    "                \n",
    "                BinKeys = np.arange(bin_widths.size) # Use as label for BinNumber column in the new dataframe\n",
    "                \n",
    "                #Bins = pd.interval_range(start=0, periods=100, freq=100, closed='left') # Recreate the momentum bins from file_df as something readable for pd.cut()\n",
    "                Bins = np.array(manual_bins)\n",
    "                \n",
    "                df['BinWidth'] = pd.cut(p, bins=Bins) # new dataframe column\n",
    "                df['BinNumber'] = pd.cut(p, bins=Bins, labels=BinKeys)\n",
    "                #df['BinNumber'] = pd.cut(p, bins=Bins).map(dict(zip(Bins,BinKeys))) # Use if Bins is defined by pd.interval_range\n",
    "                \n",
    "                BinNumber = df['BinNumber'].values # Collect the Bin Numbers into a numpy array\n",
    "                BinNumber = BinNumber.astype('int64') # Insures the bin numbers are integers\n",
    "            \n",
    "                WeightMatching = wgts[BinNumber] # Match 'wgts' with corresponding p bin using the bin number\n",
    "                Weights = weights*WeightMatching # Include 'wgts' with the previously defined 'weights'\n",
    "            else:\n",
    "                Weights = weights # No mistag rates, no change to weights\n",
    "            ###---------------------------------------------------------------------------------------------###\n",
    "            output['cutflow'][ilabel] += np.sum(icat)\n",
    "          \n",
    "            output['ttbarmass'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                ttbarmass=ttbarmass[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jetpt'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetpt=jetpt[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['probept'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetpt=pT[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['probep'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetp=p[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jeteta'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jeteta=jeteta[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jetphi'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetphi=jetphi[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jety'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jety=jety[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jetdy'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetdy=jetdy[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['numerator'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetp=numerator[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['denominator'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetp=denominator[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['deepTag_TvsQCD'].fill(dataset=dataset, anacat=ilabel,\n",
    "                                          jetpt=pT[icat],\n",
    "                                          tagger=deepTag[icat],\n",
    "                                          weight=Weights[icat])\n",
    "            output['deepTagMD_TvsQCD'].fill(dataset=dataset, anacat=ilabel,\n",
    "                                            jetpt=pT[icat],\n",
    "                                            tagger=deepTagMD[icat],\n",
    "                                            weight=Weights[icat])\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesets = {\n",
    "    #'TTbar':ttbarfiles,\n",
    "    #'QCD':qcdfiles\n",
    "    #'JetHT':jetdatafiles,\n",
    "    'JetHT2016_Data':jetdatafiles2016\n",
    "    #'JetHT2017_Data':jetdatafiles2017,\n",
    "    #'JetHT2018_Data':jetdatafiles2018\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JetHT2016_Data\n",
      "['At0bcen', 'At0bfwd', 'At1bcen', 'At1bfwd', 'At2bcen', 'At2bfwd', 'at0bcen', 'at0bfwd', 'at1bcen', 'at1bfwd', 'at2bcen', 'at2bfwd', '0t0bcen', '0t0bfwd', '0t1bcen', '0t1bfwd', '0t2bcen', '0t2bfwd', '1t0bcen', '1t0bfwd', '1t1bcen', '1t1bfwd', '1t2bcen', '1t2bfwd', '2t0bcen', '2t0bfwd', '2t1bcen', '2t1bfwd', '2t2bcen', '2t2bfwd']\n",
      "[############                            ] | 30% Completed |  7min 58.2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1417, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/ssl.py\", line 1139, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 711, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1498, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1449, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 626, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 656, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 139, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 747, in _handle_events\n",
      "    self.close(exc_info=e)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 626, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 656, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1417, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/ssl.py\", line 1139, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 711, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1498, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1449, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 626, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 656, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 139, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 747, in _handle_events\n",
      "    self.close(exc_info=e)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 626, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 656, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1417, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/ssl.py\", line 1139, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 711, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1498, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1449, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 626, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 656, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 139, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 747, in _handle_events\n",
      "    self.close(exc_info=e)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 626, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 656, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1417, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/ssl.py\", line 1139, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 711, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1498, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1449, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 626, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 656, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 139, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 747, in _handle_events\n",
      "    self.close(exc_info=e)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 626, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 656, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1417, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/ssl.py\", line 1139, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 711, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1498, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1449, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 626, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 656, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 139, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 747, in _handle_events\n",
      "    self.close(exc_info=e)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 626, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 656, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "reduce-656c266387d978198dec39067824016c",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f8f42a3940bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                           \u001b[0;34m'skipbadfiles'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                           'workers': 4},\n\u001b[0;32m---> 26\u001b[0;31m                                       \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                                       \u001b[0;31m#chunksize=Chunk[0], maxchunks=Chunk[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                      )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mrun_uproot_job\u001b[0;34m(fileset, treename, processor_instance, executor, executor_args, pre_executor, pre_args, chunksize, maxchunks, metadata_cache)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     }\n\u001b[1;32m   1067\u001b[0m     \u001b[0mexe_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m     \u001b[0mexecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexe_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[0mwrapped_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chunks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mdask_executor\u001b[0;34m(items, function, accumulator, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# FIXME: fancy widget doesn't appear, have to live with boring pbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mprogress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m     \u001b[0maccumulator\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_maybe_decompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cancelled\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCancelledError\u001b[0m: reduce-656c266387d978198dec39067824016c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "tstart = time.time()\n",
    "\n",
    "outputs_unweighted = {}\n",
    "\n",
    "seed = 1234567890\n",
    "prng = RandomState(seed)\n",
    "Chunk = [100000, 400] # [chunksize, maxchunks]\n",
    "\n",
    "for name,files in filesets.items(): \n",
    "    \n",
    "\n",
    "    print(name)\n",
    "    output = processor.run_uproot_job({name:files},\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=TTbarResProcessor(UseLookUpTables=False,\n",
    "                                                                          prng=prng),\n",
    "                                      executor=processor.dask_executor,\n",
    "                                      #executor=processor.iterative_executor,\n",
    "                                      #executor=processor.futures_executor,\n",
    "                                      executor_args={\n",
    "                                          'client': client, \n",
    "                                          'nano':False, \n",
    "                                          'flatten':True, \n",
    "                                          'skipbadfiles':False,\n",
    "                                          'workers': 4},\n",
    "                                      chunksize=100000\n",
    "                                      #chunksize=Chunk[0], maxchunks=Chunk[1]\n",
    "                                     )\n",
    "\n",
    "    elapsed = time.time() - tstart\n",
    "    outputs_unweighted[name] = output\n",
    "    print(output)\n",
    "    util.save(output, 'TTbarResCoffea_' + name + '_unweighted_output.coffea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Elapsed time = ', elapsed, ' sec.')\n",
    "print('Elapsed time = ', elapsed/60., ' min.')\n",
    "print('Elapsed time = ', elapsed/3600., ' hrs.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name,output in outputs_unweighted.items(): \n",
    "    print(\"-------Unweighted \" + name + \"--------\")\n",
    "    for i,j in output['cutflow'].items():        \n",
    "        print( '%20s : %12d' % (i,j) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(mypath):\n",
    "    '''Creates a directory. equivalent to using mkdir -p on the command line'''\n",
    "\n",
    "    from errno import EEXIST\n",
    "    from os import makedirs,path\n",
    "\n",
    "    try:\n",
    "        makedirs(mypath)\n",
    "    except OSError as exc: # Python >2.5\n",
    "        if exc.errno == EEXIST and path.isdir(mypath):\n",
    "            pass\n",
    "        else: raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoesDirectoryExist(mypath): #extra precaution (Probably overkill...)\n",
    "    '''Checks to see if Directory exists before running mkdir_p'''\n",
    "    import os.path\n",
    "    from os import path\n",
    "    \n",
    "    if path.exists(mypath):\n",
    "        pass\n",
    "    else:\n",
    "        mkdir_p(mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import re # regular expressions\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- Reiterate categories ---- #\n",
    "ttagcats = [\"at\", \"0t\", \"1t\", \"2t\"]\n",
    "btagcats = [\"0b\", \"1b\", \"2b\"]\n",
    "ycats = ['cen', 'fwd']\n",
    "\n",
    "list_of_cats = [ t+b+y for t,b,y in itertools.product( ttagcats, btagcats, ycats) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maindirectory = os.getcwd() # changes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ---------------- CREATES MISTAG PLOTS ---------------- \"\"\"\n",
    "# ---- Only Use This Cell When LookUp Tables Are Not In Use (i.e. UseLookUpTables = False) ---- #\n",
    "\n",
    "SaveDirectory = maindirectory + '/MistagPlots/'\n",
    "DoesDirectoryExist(SaveDirectory) # no need to create the directory several times\n",
    "\n",
    "# Function sqrt(x)\n",
    "def forward(x):\n",
    "    return x**(1/2)\n",
    "\n",
    "\n",
    "def inverse(x):\n",
    "    return x**2\n",
    "\n",
    "print(SaveDirectory)\n",
    "for iset in filesets:\n",
    "    for icat in list_of_cats:\n",
    "        print(iset)\n",
    "        print(icat)\n",
    "        title = iset + ' mistag ' + icat\n",
    "        filename = 'mistag_' + iset + '_' + icat + '.' + 'png'\n",
    "        print(outputs_unweighted[iset]['numerator'])\n",
    "        Numerator = outputs_unweighted[iset]['numerator'].integrate('anacat', icat).integrate('dataset', iset)\n",
    "        Denominator = outputs_unweighted[iset]['denominator'].integrate('anacat', icat).integrate('dataset', iset)\n",
    "        print(Numerator)\n",
    "        print(Denominator)\n",
    "        mistag = hist.plotratio(num = Numerator, denom = Denominator,\n",
    "                                error_opts={'marker': '.', 'markersize': 10., 'color': 'k', 'elinewidth': 1},\n",
    "                                unc = 'num')\n",
    "        plt.title(title)\n",
    "        plt.ylim(bottom = 0, top = 0.12)\n",
    "        plt.xlim(left = 100, right = 2500)\n",
    "        #plt.xticks(np.array([0, 500, 600, 700]))\n",
    "        #mistag.set_xscale('function', functions=(forward, inverse))\n",
    "        #mistag.set_xscale('log')\n",
    "        \n",
    "        #plt.savefig(SaveDirectory+filename, bbox_inches=\"tight\")\n",
    "        #print(filename + ' saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nevts2016 = 625516390. # from dasgoclient\n",
    "Nevts2016_totalchunks = Nevts2016 / Chunk[0]\n",
    "Nevts2016_sf = Nevts2016_totalchunks / Chunk[1]\n",
    "\n",
    "Nevts2017 = 410461585. # from dasgoclient\n",
    "Nevts2017_totalchunks = Nevts2017 / Chunk[0]\n",
    "Nevts2017_sf = Nevts2017_totalchunks / Chunk[1]\n",
    "\n",
    "Nevts2018 = 676328827. # from dasgoclient\n",
    "Nevts2018_totalchunks = Nevts2018 / Chunk[0]\n",
    "Nevts2018_sf = Nevts2018_totalchunks / Chunk[1]\n",
    "\n",
    "Nevts = Nevts2016 + Nevts2017 + Nevts2018\n",
    "Nevts_totalchunks = Nevts / Chunk[0]\n",
    "Nevts_sf = Nevts_totalchunks / Chunk[1]\n",
    "\n",
    "print(Nevts2016_sf)\n",
    "print(Nevts2017_sf)\n",
    "print(Nevts2018_sf)\n",
    "print(Nevts_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ---------------- Luminosities, Cross Sections, Scale-Factors ---------------- \"\"\" \n",
    "Lum2016 = 35920. # pb^-1 from https://twiki.cern.ch/twiki/bin/viewauth/CMS/PdmVAnalysisSummaryTable\n",
    "Lum2017 = 41530.\n",
    "Lum2018 = 59740.\n",
    "Lum     = 137190.\n",
    "\n",
    "ttbar_BR = 0.457 # 0.442 from PDG 2018\n",
    "ttbar_xs = 831.76 * ttbar_BR  #pb\n",
    "ttbar2016_sf = ttbar_xs*Lum2016/142155064.\n",
    "ttbar2017_sf = ttbar_xs*Lum2017/142155064.\n",
    "ttbar2018_sf = ttbar_xs*Lum2018/142155064.\n",
    "ttbar_sf = ttbar_xs*Lum/142155064.\n",
    "print(ttbar2016_sf)\n",
    "print(ttbar2017_sf)\n",
    "print(ttbar2018_sf)\n",
    "print(ttbar_sf)\n",
    "\n",
    "qcd_xs = 1370000000.0 #pb From https://cms-gen-dev.cern.ch/xsdb\n",
    "#qcd_sf = qcd_xs*Lum/18455107."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ---------------- CREATE LOOK UP TABLE .CSV FILES ---------------- \"\"\"\n",
    "# ---- Only Use This Cell When LookUp Tables Are Not In Use (i.e. UseLookUpTables = False) ---- #\n",
    "from collections import defaultdict\n",
    "\n",
    "runLUTS = True \n",
    "\n",
    "def multi_dict(K, type): # definition from https://www.geeksforgeeks.org/python-creating-multidimensional-dictionary/\n",
    "    if K == 1: \n",
    "        return defaultdict(type) \n",
    "    else: \n",
    "        return defaultdict(lambda: multi_dict(K-1, type))\n",
    "    \n",
    "luts = {}\n",
    "luts = multi_dict(2, str)\n",
    "\n",
    "if runLUTS : \n",
    "\n",
    "    SaveDirectory = maindirectory + '/LookupTables/'\n",
    "    DoesDirectoryExist(SaveDirectory)\n",
    "\n",
    "    for iset in filesets:\n",
    "        if iset != 'TTbar' or iset != 'QCD': # if JetHT filesets are found...\n",
    "            print('\\t\\tfileset: ' + iset + '\\n*****************************************************\\n')\n",
    "            for icat in list_of_cats:\n",
    "                title = iset + ' mistag ' + icat\n",
    "                filename = 'mistag_' + iset + '_' + icat + '.' + 'csv'\n",
    "                \n",
    "                # ---- Info from TTbar ---- #\n",
    "                #Numerator_tt = outputs_unweighted['TTbar']['numerator'].integrate('anacat',icat).integrate('dataset','TTbar')\n",
    "                #Denominator_tt = outputs_unweighted['TTbar']['denominator'].integrate('anacat',icat).integrate('dataset','TTbar')\n",
    "                #N_vals_tt = np.zeros( np.size(Numerator_tt.values()[()]) ) #Numerator_tt.values()[()]\n",
    "                #D_vals_tt = np.zeros( np.size(Denominator_tt.values()[()]) ) #Denominator_tt.values()[()]\n",
    "                \n",
    "                # ---- Info from JetHT datasets ---- #\n",
    "                Numerator = outputs_unweighted[iset]['numerator'].integrate('anacat',icat).integrate('dataset',iset)\n",
    "                Denominator = outputs_unweighted[iset]['denominator'].integrate('anacat',icat).integrate('dataset',iset)\n",
    "                N_vals = Numerator.values()[()]\n",
    "                D_vals = Denominator.values()[()]\n",
    "                \n",
    "                # ---- Properly scale chunks of data and ttbar MC according to year of dataset used---- #\n",
    "                if '2016' in iset:\n",
    "                    N_vals *= Nevts2016_sf \n",
    "                    D_vals *= Nevts2016_sf\n",
    "                    #N_vals_tt *= ttbar2016_sf\n",
    "                    #D_vals_tt *= ttbar2016_sf\n",
    "                elif '2017' in iset:\n",
    "                    N_vals *= Nevts2017_sf \n",
    "                    D_vals *= Nevts2017_sf\n",
    "                    #N_vals_tt *= ttbar2017_sf\n",
    "                    #D_vals_tt *= ttbar2017_sf\n",
    "                elif '2018' in iset:\n",
    "                    N_vals *= Nevts2018_sf \n",
    "                    D_vals *= Nevts2018_sf\n",
    "                    #N_vals_tt *= ttbar2018_sf\n",
    "                    #D_vals_tt *= ttbar2018_sf\n",
    "                else:\n",
    "                    N_vals *= Nevts_sf \n",
    "                    D_vals *= Nevts_sf\n",
    "                    #N_vals_tt *= ttbar_sf\n",
    "                    #D_vals_tt *= ttbar_sf\n",
    "                \n",
    "                # ---- Subtract ttbar MC probe momenta from datasets' ---- #\n",
    "                N_vals_diff = N_vals #np.abs(N_vals-N_vals_tt)\n",
    "                D_vals_diff = D_vals #np.abs(D_vals-D_vals_tt)\n",
    "\n",
    "                print(N_vals_diff)\n",
    "                print(D_vals_diff)\n",
    "                print()\n",
    "\n",
    "                # ---- Define Mistag values ---- #\n",
    "                mistag_vals = np.where(D_vals_diff > 0, N_vals_diff/D_vals_diff, 0)\n",
    "\n",
    "                p_vals = [] # Momentum values\n",
    "                for iden in Numerator.identifiers('jetp'):\n",
    "                    p_vals.append(iden)\n",
    "\n",
    "\n",
    "                print('fileset:  ' + iset)\n",
    "                print('category: ' + icat)\n",
    "                print('________________________________________________\\n')\n",
    "\n",
    "                d = {'p': p_vals, 'M(p)': mistag_vals}\n",
    "\n",
    "                #print(\"p vals = \", p_vals)\n",
    "                print(\"d vals = \", d)\n",
    "                print()\n",
    "                df = pd.DataFrame(data=d)\n",
    "                luts[iset][icat] = df\n",
    "\n",
    "                with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
    "                    print(df)\n",
    "                print('\\n')\n",
    "\n",
    "                df.to_csv(SaveDirectory+filename) # use later to collect bins and weights for re-scaling\n",
    "        else:\n",
    "            for icat in list_of_cats:\n",
    "                Numerator = outputs_unweighted[iset]['numerator'].integrate('anacat',icat).integrate('dataset',iset)\n",
    "                Denominator = outputs_unweighted[iset]['denominator'].integrate('anacat',icat).integrate('dataset',iset)\n",
    "                N_vals = Numerator.values()[()]\n",
    "                D_vals = Denominator.values()[()]\n",
    "                print(N_vals)\n",
    "                print(D_vals)\n",
    "                print()\n",
    "                mistag_vals = np.where(D_vals > 0, N_vals/D_vals, 0)\n",
    "                \n",
    "                p_vals = [] # Momentum values\n",
    "                for iden in Numerator.identifiers('jetp'):\n",
    "                    p_vals.append(iden)\n",
    "                print('fileset:  ' + iset)\n",
    "                print('category: ' + icat)\n",
    "                print('________________________________________________\\n')\n",
    "                d = {'p': p_vals, 'M(p)': mistag_vals}\n",
    "\n",
    "                print(\"d vals = \", d)\n",
    "                print()\n",
    "                df = pd.DataFrame(data=d)\n",
    "                luts[iset][icat] = df\n",
    "\n",
    "                with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
    "                    print(df)\n",
    "                print('\\n')\n",
    "\n",
    "                df.to_csv(SaveDirectory+filename) # use later to collect bins and weights for re-scaling\n",
    "else :\n",
    "    for iset in filesets:\n",
    "        print('\\t\\tfileset: ' + iset + '\\n*****************************************************\\n')\n",
    "        for icat in list_of_cats:\n",
    "            title = iset + ' mistag ' + icat\n",
    "            filename = 'mistag_' + iset + '_' + icat + '.' + 'csv'\n",
    "            luts[iset][icat] = pd.read_csv(filename)\n",
    "print(luts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Events/s:\", output['cutflow']['all events']/elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesets = {\n",
    "    #'TTbar':ttbarfiles,\n",
    "    #'QCD':qcdfiles,\n",
    "    #'JetHT':jetdatafiles,\n",
    "    'JetHT2016_Data':jetdatafiles2016\n",
    "    #'JetHT2017_Data':jetdatafiles2017,\n",
    "    #'JetHT2018_Data':jetdatafiles2018\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart = time.time()\n",
    "\n",
    "outputs_weighted = {}\n",
    "prng = RandomState(seed)\n",
    "Chunk = [100000, 400] # [chunksize, maxchunks]\n",
    "\n",
    "for name,files in filesets.items(): \n",
    "    \n",
    "\n",
    "    print(name)\n",
    "    output = processor.run_uproot_job({name:files},\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=TTbarResProcessor(UseLookUpTables=True,\n",
    "                                                                           lu=luts,\n",
    "                                                                           prng=prng),\n",
    "                                      executor=processor.dask_executor,\n",
    "                                      #executor=processor.iterative_executor,\n",
    "                                      #executor=processor.futures_executor,\n",
    "                                      executor_args={\n",
    "                                          'client': client, \n",
    "                                          'nano':False, \n",
    "                                          'flatten':True, \n",
    "                                          'skipbadfiles':False,\n",
    "                                          'workers': 2},\n",
    "                                      #chunksize=Chunk[0], maxchunks=Chunk[1]\n",
    "                                     )\n",
    "\n",
    "    elapsed = time.time() - tstart\n",
    "    outputs_weighted[name] = output\n",
    "    print(output)\n",
    "    util.save(output, 'TTbarResCoffea_' + name + '_weighted_output.coffea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name,output in outputs_weighted.items(): \n",
    "    print(\"-------weighted \" + name + \"--------\")\n",
    "    for i,j in output['cutflow'].items():        \n",
    "        print( '%20s : %12d' % (i,j) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
