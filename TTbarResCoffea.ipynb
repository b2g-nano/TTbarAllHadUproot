{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TTbarResCoffea` Notebook to perform the data-driven mistag-rate-based ttbar hadronic analysis. \n",
    "This module must be run twice: \n",
    "   1. Make the mistag rate in the \"anti-tag and probe\" selection \n",
    "and the expectation in the signal region from MC,\n",
    "   1. Applies that mistag rate and the mod-mass procedure to the single-tag selection. \n",
    "\n",
    "These are all done in bins of\n",
    "b-tag categories (0, 1, $\\ge 2$) and rapidity ($|y| \\le 1.0$, $|y| > 1.0$).\n",
    "The signal region is two top-tagged jets. \n",
    "The background estimate is the single-tag selection weighted by the mistag rate from the\n",
    "\"anti-tag and probe\" region, with the mass of the weighted jet set to a random\n",
    "value from QCD MC in the 1-ttag region. \n",
    "\n",
    "\n",
    "The preselection is:\n",
    "- AK4-based $H_{T} > 1100$ GeV (to be on the trigger plateau). \n",
    "- $\\ge 2$ AK8 jets with AK8 $p_{T} > 400$ GeV and $|y| < 2.5$, loose jet ID applied from matched AK4 jets\n",
    "\n",
    "The 1-tag selection adds:\n",
    "- $\\ge 1$ AK8 jet with top tagging applied to randomly-assigned tag jet. \n",
    "\n",
    "\n",
    "The anti-tag selection is disjoint from the 1-tag selection:\n",
    "- $\\ge 1$ AK8 jet with top tagging VETO applied to randomly-assigned tag jet. \n",
    "\n",
    "\n",
    "The 2-tag selection is:\n",
    "- $\\ge 2$ AK8 jets with top tagging applied to both leading jets. \n",
    "\n",
    "\n",
    "The ttbar candidate mass assumes the two leading top-tagged jets are the top quarks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- All Datasets used for analysis and dasgoclient searches ---- #\n",
    "\n",
    "\"\"\"\n",
    "/JetHT/Run2016B_ver1-Nano25Oct2019_ver1-v1/NANOAOD\n",
    "/JetHT/Run2016B_ver2-Nano25Oct2019_ver2-v1/NANOAOD\n",
    "/JetHT/Run2016C-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2016D-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2016E-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2016F-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2016G-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2016H-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2017B-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2017C-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2017D-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2017E-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2017F-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2018A-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2018B-Nano25Oct2019-v1/NANOAOD\n",
    "/JetHT/Run2018C-Nano25Oct2019-v2/NANOAOD\n",
    "/JetHT/Run2018D-Nano25Oct2019_ver2-v1/NANOAOD\n",
    "\n",
    "/TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8/RunIIAutumn18NanoAODv7-Nano02Apr2020_102X_upgrade2018_realistic_v21_ext1-v1/NANOAODSIM\n",
    "/QCD_Pt-15to7000_TuneCP5_Flat_13TeV_pythia8/RunIIFall17NanoAODv7-PU2017_12Apr2018_Nano02Apr2020_102X_mc2017_realistic_v8-v1/NANOAODSIM\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import scipy.stats as ss\n",
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor\n",
    "from coffea import util\n",
    "from awkward import JaggedArray\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrootdstr1 = 'root://cmseos.fnal.gov//'\n",
    "xrootdstr2 = 'root://cmsxrootd.fnal.gov//'\n",
    "xrootdstr3 = 'root://cmsxrootd-site.fnal.gov/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcdfilename = 'QCD.txt'\n",
    "with open(qcdfilename) as f:\n",
    "    qcdfiles = [xrootdstr2 + s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttbarfilename = 'TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8.txt'\n",
    "with open(ttbarfilename) as f:\n",
    "    ttbarfiles = [xrootdstr2 + s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zprimeDMfilename = 'ZprimeDMToTTbar.txt'\n",
    "with open(zprimeDMfilename) as f:\n",
    "    zprimeDMfiles = [xrootdstr2 + s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetdatafilename = 'JetHT_Data.txt'\n",
    "with open(jetdatafilename) as f:\n",
    "    jetdatafiles = [xrootdstr2 + s.strip() for s in f.readlines()[::3]] # Every third datafile\n",
    "with open(jetdatafilename) as g:\n",
    "    jetdatafiles2016 = [xrootdstr2 + s.strip() for s in g.readlines() if \"/store/data/Run2016\" in s]\n",
    "with open(jetdatafilename) as h:\n",
    "    jetdatafiles2017 = [xrootdstr2 + s.strip() for s in h.readlines() if \"/store/data/Run2017\" in s]\n",
    "with open(jetdatafilename) as i:\n",
    "    jetdatafiles2018 = [xrootdstr2 + s.strip() for s in i.readlines() if \"/store/data/Run2018\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(jetdatafiles[2]) # Test to see if correct files are collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from columnservice.client import ColumnClient\n",
    "cc = ColumnClient(\"coffea-dask.fnal.gov\")\n",
    "client = cc.get_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btag0_bins = []\n",
    "ix = 400\n",
    "while ix <= 7000:\n",
    "    btag0_bins.append(ix)\n",
    "    if ix < 1000:\n",
    "        ix += 100\n",
    "    elif ix < 1200:\n",
    "        ix += 200\n",
    "    elif ix < 1500:\n",
    "        ix += 300\n",
    "    elif ix < 2000:\n",
    "        ix += 500\n",
    "    elif ix < 3000:\n",
    "        ix += 1000\n",
    "    else:\n",
    "        ix += 4000\n",
    "print(btag0_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_bins = [400, 500, 600, 800, 1000, 1500, 2000, 3000, 7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"@TTbarResAnaHadronic Package to perform the data-driven mistag-rate-based ttbar hadronic analysis. \n",
    "\"\"\"\n",
    "class TTbarResProcessor(processor.ProcessorABC):\n",
    "    def __init__(self, prng, htCut=950., minMSD=105., maxMSD=210., tau32Cut=0.65, ak8PtMin=400., bdisc=0.8484,\n",
    "                writePredDist=True,isData=True,year=2019, UseLookUpTables=False, lu=None, \n",
    "                ModMass=False, RandomDebugMode=False):\n",
    "        \n",
    "        self.prng = prng\n",
    "        self.htCut = htCut\n",
    "        self.minMSD = minMSD\n",
    "        self.maxMSD = maxMSD\n",
    "        self.tau32Cut = tau32Cut\n",
    "        self.ak8PtMin = ak8PtMin\n",
    "        self.bdisc = bdisc\n",
    "        self.writePredDist = writePredDist\n",
    "        self.writeHistFile = True\n",
    "        self.isData = isData\n",
    "        self.year=year\n",
    "        self.UseLookUpTables = UseLookUpTables\n",
    "        self.ModMass = ModMass\n",
    "        self.RandomDebugMode = RandomDebugMode\n",
    "        self.lu = lu # Look Up Tables\n",
    "        \n",
    "        self.ttagcats = [\"Pt\", \"at\", \"pret\", \"0t\", \"1t\", \"1t+2t\", \"2t\", \"0t+1t+2t\"] #anti-tag+probe, anti-tag, pre-tag, 0, 1, >=1, 2 ttags, any t-tag\n",
    "        self.btagcats = [\"0b\", \"1b\", \"2b\"]   # 0, 1, >=2 btags\n",
    "        self.ycats = ['cen', 'fwd']          # Central and forward\n",
    "        # Combine categories like \"0bcen\", \"0bfwd\", etc:\n",
    "        self.anacats = [ t+b+y for t,b,y in itertools.product( self.ttagcats, self.btagcats, self.ycats) ]\n",
    "        print(self.anacats)\n",
    "        \n",
    "        dataset_axis = hist.Cat(\"dataset\", \"Primary dataset\")\n",
    "        cats_axis = hist.Cat(\"anacat\", \"Analysis Category\")\n",
    "        \n",
    "        jetmass_axis = hist.Bin(\"jetmass\", r\"Jet $m$ [GeV]\", 50, 0, 500)\n",
    "        jetpt_axis = hist.Bin(\"jetpt\", r\"Jet $p_{T}$ [GeV]\", 50, 0, 5000)\n",
    "        ttbarmass_axis = hist.Bin(\"ttbarmass\", r\"$m_{t\\bar{t}}$ [GeV]\", 50, 0, 5000)\n",
    "        jeteta_axis = hist.Bin(\"jeteta\", r\"Jet $\\eta$\", 50, -5, 5)\n",
    "        jetphi_axis = hist.Bin(\"jetphi\", r\"Jet $\\phi$\", 50, -np.pi, np.pi)\n",
    "        jety_axis = hist.Bin(\"jety\", r\"Jet $y$\", 50, -3, 3)\n",
    "        jetdy_axis = hist.Bin(\"jetdy\", r\"Jet $\\Delta y$\", 50, 0, 5)\n",
    "        #jetp_axis = hist.Bin(\"jetp\", r\"Jet $p$ [GeV]\", 100, 0, 10000)\n",
    "        manual_axis = hist.Bin(\"jetp\", r\"Jet Momentum [GeV]\", manual_bins)\n",
    "        tagger_axis = hist.Bin(\"tagger\", r\"deepTag\", 50, 0, 1)\n",
    "        tau32_axis = hist.Bin(\"tau32\", r\"$\\tau_3/\\tau_2$\", 50, 0, 2)\n",
    "        \n",
    "        subjetmass_axis = hist.Bin(\"subjetmass\", r\"SubJet $m$ [GeV]\", 50, 0, 500)\n",
    "        subjetpt_axis = hist.Bin(\"subjetpt\", r\"SubJet $p_{T}$ [GeV]\", 50, 0, 2000)\n",
    "        subjeteta_axis = hist.Bin(\"subjeteta\", r\"SubJet $\\eta$\", 50, -4, 4)\n",
    "        subjetphi_axis = hist.Bin(\"subjetphi\", r\"SubJet $\\phi$\", 50, -np.pi, np.pi)\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'ttbarmass': hist.Hist(\"Counts\", dataset_axis, cats_axis, ttbarmass_axis),\n",
    "            \n",
    "            'jetmass':         hist.Hist(\"Counts\", dataset_axis, cats_axis, jetmass_axis),\n",
    "            'SDmass':          hist.Hist(\"Counts\", dataset_axis, cats_axis, jetmass_axis),\n",
    "            'SDmass_precat':   hist.Hist(\"Counts\", dataset_axis, jetpt_axis, jetmass_axis),\n",
    "            \n",
    "            'jetpt':     hist.Hist(\"Counts\", dataset_axis, cats_axis, jetpt_axis),\n",
    "            'jeteta':    hist.Hist(\"Counts\", dataset_axis, cats_axis, jeteta_axis),\n",
    "            'jetphi':    hist.Hist(\"Counts\", dataset_axis, cats_axis, jetphi_axis),\n",
    "            \n",
    "            'probept':   hist.Hist(\"Counts\", dataset_axis, cats_axis, jetpt_axis),\n",
    "            'probep':    hist.Hist(\"Counts\", dataset_axis, cats_axis, manual_axis),\n",
    "            \n",
    "            'jety':      hist.Hist(\"Counts\", dataset_axis, cats_axis, jety_axis),\n",
    "            'jetdy':     hist.Hist(\"Counts\", dataset_axis, cats_axis, jetdy_axis),\n",
    "            \n",
    "            'deepTag_TvsQCD':   hist.Hist(\"Counts\", dataset_axis, cats_axis, jetpt_axis, tagger_axis),\n",
    "            'deepTagMD_TvsQCD': hist.Hist(\"Counts\", dataset_axis, cats_axis, jetpt_axis, tagger_axis),\n",
    "            \n",
    "            'tau32':          hist.Hist(\"Counts\", dataset_axis, cats_axis, tau32_axis),\n",
    "            'tau32_2D':       hist.Hist(\"Counts\", dataset_axis, cats_axis, jetpt_axis, tau32_axis),\n",
    "            'tau32_precat': hist.Hist(\"Counts\", dataset_axis, jetpt_axis, tau32_axis),\n",
    "            \n",
    "            'subjetmass':   hist.Hist(\"Counts\", dataset_axis, cats_axis, subjetmass_axis),\n",
    "            'subjetpt':     hist.Hist(\"Counts\", dataset_axis, cats_axis, subjetpt_axis),\n",
    "            'subjeteta':    hist.Hist(\"Counts\", dataset_axis, cats_axis, subjeteta_axis),\n",
    "            'subjetphi':    hist.Hist(\"Counts\", dataset_axis, cats_axis, subjetphi_axis),\n",
    "            \n",
    "            'numerator':   hist.Hist(\"Counts\", dataset_axis, cats_axis, manual_axis),\n",
    "            'denominator': hist.Hist(\"Counts\", dataset_axis, cats_axis, manual_axis),\n",
    "            \n",
    "            'cutflow': processor.defaultdict_accumulator(int),\n",
    "            \n",
    "        })\n",
    "\n",
    "            \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, df):\n",
    "        \n",
    "        output = self.accumulator.identity()\n",
    "        #dataset = events.metadata['dataset']\n",
    "        \n",
    "        # ---- Define dataset ---- #\n",
    "        dataset = df['dataset'] #coffea.processor.LazyDataFrame\n",
    "        Dataset_info = df.available #list of available columns in LazyDataFrame object (Similar to 'Events->Show()' command in ROOT)\n",
    "        \n",
    "        # ---- Get triggers from Dataset_info ---- #\n",
    "        #triggers = [itrig for itrig in Dataset_info if 'HLT_PFHT' in itrig]\n",
    "        #AK8triggers = [itrig for itrig in Dataset_info if 'HLT_AK8PFHT' in itrig]\n",
    "\n",
    "        # ---- Find numeric values in trigger strings ---- #\n",
    "        #triggers_cut1 = [sub.split('PFHT')[1] for sub in triggers] # Remove string characters from left of number\n",
    "        #triggers_cut2 = [sub.split('_')[0] for sub in triggers_cut1] # Remove string characters from right of number\n",
    "        #isTriggerValue = [val.isnumeric() for val in triggers_cut2] # Boolean -> if string is only a number\n",
    "        #triggers_cut2 = np.where(isTriggerValue, triggers_cut2, 0) # If string is not a number, replace with 0\n",
    "        #triggers_vals = [int(val) for val in triggers_cut2] # Convert string numbers to integers\n",
    "        \n",
    "        #AK8triggers_cut1 = [sub.split('HT')[1] for sub in AK8triggers]\n",
    "        #AK8triggers_cut2 = [sub.split('_')[0] for sub in AK8triggers_cut1]\n",
    "        #isAK8TriggerValue = [val.isnumeric() for val in AK8triggers_cut2]\n",
    "        #AK8triggers_cut2 = np.where(isAK8TriggerValue, AK8triggers_cut2, 0)\n",
    "        #AK8triggers_vals = [int(val) for val in AK8triggers_cut2]\n",
    "        \n",
    "        # ---- Find Largest and Second Largest Value ---- #\n",
    "        #triggers_vals.sort(reverse = True)\n",
    "        #AK8triggers_vals.sort(reverse = True)\n",
    "        \n",
    "        #triggers_vals1 = str(triggers_vals[0])\n",
    "        #triggers_vals2 = str(triggers_vals[1])\n",
    "        #AK8triggers_vals1 = str(AK8triggers_vals[0])\n",
    "        #AK8triggers_vals2 = str(AK8triggers_vals[1])\n",
    "        \n",
    "        # ---- Define strings for the selected triggers ---- #\n",
    "        #HLT_trig1_str = [itrig for itrig in triggers if (triggers_vals1) in itrig][0]\n",
    "        #HLT_trig2_str = [itrig for itrig in triggers if (triggers_vals2) in itrig][0]\n",
    "        #HLT_AK8_trig1_str = [itrig for itrig in AK8triggers if (AK8triggers_vals1) in itrig][0]\n",
    "        #HLT_AK8_trig2_str = [itrig for itrig in AK8triggers if (AK8triggers_vals2) in itrig][0]\n",
    "        \n",
    "        # ---- Define HLT triggers to be used ---- #\n",
    "        #HLT_trig1 = df[HLT_trig1_str]\n",
    "        #HLT_trig2 = df[HLT_trig2_str]\n",
    "        #HLT_AK8_trig1 = df[HLT_AK8_trig1_str]\n",
    "        #HLT_AK8_trig2 = df[HLT_AK8_trig2_str]\n",
    "       \n",
    "        \n",
    "        # ---- Define AK8 Jets as FatJets ---- #\n",
    "        FatJets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nFatJet'],\n",
    "            pt=df['FatJet_pt'],\n",
    "            eta=df['FatJet_eta'],\n",
    "            phi=df['FatJet_phi'],\n",
    "            mass=df['FatJet_mass'],\n",
    "            area=df['FatJet_area'],\n",
    "            msoftdrop=df['FatJet_msoftdrop'],\n",
    "            jetId=df['FatJet_jetId'],\n",
    "            tau1=df['FatJet_tau1'],\n",
    "            tau2=df['FatJet_tau2'],\n",
    "            tau3=df['FatJet_tau3'],\n",
    "            tau4=df['FatJet_tau4'],\n",
    "            n3b1=df['FatJet_n3b1'],\n",
    "            btagDeepB=df['FatJet_btagDeepB'],\n",
    "            btagCSVV2=df['FatJet_btagCSVV2'],\n",
    "            deepTag_TvsQCD=df['FatJet_deepTag_TvsQCD'],\n",
    "            deepTagMD_TvsQCD=df['FatJet_deepTagMD_TvsQCD'],\n",
    "            subJetIdx1=df['FatJet_subJetIdx1'],\n",
    "            subJetIdx2=df['FatJet_subJetIdx2']\n",
    "            )\n",
    "        \n",
    "        # ---- Define AK4 jets as Jets ---- #\n",
    "        Jets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nJet'],\n",
    "            pt=df['Jet_pt'],\n",
    "            eta=df['Jet_eta'],\n",
    "            phi=df['Jet_phi'],\n",
    "            mass=df['Jet_mass'],\n",
    "            area=df['Jet_area']\n",
    "            )\n",
    "        # ---- Define SubJets ---- #\n",
    "        SubJets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nSubJet'],\n",
    "            pt=df['SubJet_pt'],\n",
    "            eta=df['SubJet_eta'],\n",
    "            phi=df['SubJet_phi'],\n",
    "            mass=df['SubJet_mass'],\n",
    "            btagDeepB=df['SubJet_btagDeepB'],\n",
    "            btagCSVV2=df['SubJet_btagCSVV2']\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # ---- Get event weights from dataset ---- #\n",
    "        if 'JetHT' in dataset: # If data is used...\n",
    "            evtweights = np.ones(FatJets.size) # set all \"data weights\" to one\n",
    "        else: # if Monte Carlo dataset is used...\n",
    "            evtweights = df[\"Generator_weight\"].reshape(-1, 1).flatten()\n",
    "        \n",
    "        # ---- Show all events ---- #\n",
    "        output['cutflow']['all events'] += FatJets.size\n",
    "        \n",
    "        # ---- Apply Trigger(s) ---- #\n",
    "        #FatJets = FatJets[HLT_AK8_trig1]\n",
    "        #evtweights = evtweights[HLT_AK8_trig1]\n",
    "        #Jets = Jets[HLT_AK8_trig1]\n",
    "        #SubJets = SubJets[HLT_AK8_trig1]\n",
    "        \n",
    "        # ---- Jets that satisfy Jet ID ---- #\n",
    "        jet_id = (FatJets.jetId > 0) # Loose jet ID\n",
    "        FatJets = FatJets[jet_id]\n",
    "        output['cutflow']['jet id'] += jet_id.any().sum()\n",
    "        \n",
    "        # ---- Apply pT Cut and Rapidity Window ---- #\n",
    "        jetkincut_index = (FatJets.pt > self.ak8PtMin) & (np.abs(FatJets.p4.rapidity) < 2.4)\n",
    "        FatJets = FatJets[ jetkincut_index ]\n",
    "        output['cutflow']['jet kin'] += jetkincut_index.any().sum()\n",
    "        \n",
    "        # ---- Find two AK8 Jets ---- #\n",
    "        twoFatJetsKin = (FatJets.counts == 2)\n",
    "        FatJets = FatJets[twoFatJetsKin]\n",
    "        evtweights = evtweights[twoFatJetsKin]\n",
    "        Jets = Jets[twoFatJetsKin]\n",
    "        SubJets = SubJets[twoFatJetsKin]\n",
    "        output['cutflow']['two FatJets and jet kin'] += twoFatJetsKin.sum()\n",
    "        \n",
    "        # ---- Apply HT Cut ---- #\n",
    "        hT = Jets.pt.sum()\n",
    "        passhT = (hT > self.htCut)\n",
    "        evtweights = evtweights[passhT]\n",
    "        FatJets = FatJets[passhT]\n",
    "        SubJets = SubJets[passhT]\n",
    "        \n",
    "        # ---- Randomly Assign AK8 Jets as TTbar Candidates 0 and 1 --- #\n",
    "        if self.RandomDebugMode == True: # 'Sudo' randomizer for consistent results\n",
    "            highPhi = FatJets.phi[:,0] > FatJets.phi[:,1]\n",
    "            highRandIndex = np.where(highPhi, 0, 1)\n",
    "            index = JaggedArray.fromcounts(np.ones(len(FatJets), dtype='i'), highRandIndex )\n",
    "        else: # Truly randomize\n",
    "            index = JaggedArray.fromcounts(np.ones(len(FatJets), dtype='i'), prng.randint(2, size=len(FatJets)))\n",
    "        jet0 = FatJets[index] #J0\n",
    "        jet1 = FatJets[1 - index] #J1\n",
    "        \n",
    "        ttbarcands = jet0.cross(jet1) #FatJets[:,0:2].distincts()\n",
    "    \n",
    "        # ---- Make sure we have at least 1 TTbar candidate pair and re-broadcast releveant arrays  ---- #\n",
    "        oneTTbar = (ttbarcands.counts >= 1)\n",
    "        output['cutflow']['>= one oneTTbar'] += oneTTbar.sum()\n",
    "        ttbarcands = ttbarcands[oneTTbar]\n",
    "        evtweights = evtweights[oneTTbar]\n",
    "        FatJets = FatJets[oneTTbar]\n",
    "        SubJets = SubJets[oneTTbar]\n",
    "         \n",
    "        # ---- Apply Delta Phi Cut for Back to Back Topology ---- #\n",
    "        dPhiCut = (ttbarcands.i0.p4.delta_phi(ttbarcands.i1.p4) > 2.1).flatten()\n",
    "        output['cutflow']['dPhi > 2.1'] += dPhiCut.sum()\n",
    "        ttbarcands = ttbarcands[dPhiCut]\n",
    "        evtweights = evtweights[dPhiCut]\n",
    "        FatJets = FatJets[dPhiCut] \n",
    "        SubJets = SubJets[dPhiCut] \n",
    "        \n",
    "        # ---- Identify subjets according to subjet ID ---- #\n",
    "        hasSubjets0 = ((ttbarcands.i0.subJetIdx1 > -1) & (ttbarcands.i0.subJetIdx2 > -1))\n",
    "        hasSubjets1 = ((ttbarcands.i1.subJetIdx1 > -1) & (ttbarcands.i1.subJetIdx2 > -1))\n",
    "        GoodSubjets = ((hasSubjets0) & (hasSubjets1)).flatten()\n",
    "   \n",
    "        ttbarcands = ttbarcands[GoodSubjets]\n",
    "        \n",
    "        SubJets = SubJets[GoodSubjets]\n",
    "        evtweights = evtweights[GoodSubjets]\n",
    "       \n",
    "        SubJet01 = SubJets[ttbarcands.i0.subJetIdx1] # FatJet i0 with subjet 1\n",
    "        SubJet02 = SubJets[ttbarcands.i0.subJetIdx2] # FatJet i0 with subjet 2\n",
    "        SubJet11 = SubJets[ttbarcands.i1.subJetIdx1] # FatJet i1 with subjet 1\n",
    "        SubJet12 = SubJets[ttbarcands.i1.subJetIdx2] # FatJet i1 with subjet 2\n",
    "        \n",
    "        # ---- Define Rapidity Regions ---- #\n",
    "        cen = np.abs(ttbarcands.i0.p4.rapidity - ttbarcands.i1.p4.rapidity) < 1.0\n",
    "        fwd = (~cen)\n",
    "        \n",
    "        # ---- CMS Top Tagger Version 2 (SD and Tau32 Cuts) ---- #\n",
    "        tau32_i0 = np.where(ttbarcands.i0.tau2>0,ttbarcands.i0.tau3/ttbarcands.i0.tau2, 0 )\n",
    "        tau32_i1 = np.where(ttbarcands.i1.tau2>0,ttbarcands.i1.tau3/ttbarcands.i1.tau2, 0 )\n",
    "        taucut_i0 = tau32_i0 < self.tau32Cut\n",
    "        taucut_i1 = tau32_i1 < self.tau32Cut\n",
    "        mcut_i0 = (self.minMSD < ttbarcands.i0.msoftdrop) & (ttbarcands.i0.msoftdrop < self.maxMSD) \n",
    "        mcut_i1 = (self.minMSD < ttbarcands.i1.msoftdrop) & (ttbarcands.i1.msoftdrop < self.maxMSD) \n",
    "\n",
    "        ttag_i0 = (taucut_i0) & (mcut_i0)\n",
    "        ttag_i1 = (taucut_i1) & (mcut_i1)\n",
    "        \n",
    "        # ---- Define \"Top Tag\" Regions ---- #\n",
    "        antitag = (~taucut_i0) & (mcut_i0) #Probe will always be ttbarcands.i1 (at)\n",
    "        antitag_probe = np.logical_and(antitag, ttag_i1) #Found an antitag and ttagged probe pair for mistag rate (Pt)\n",
    "        pretag =  ttag_i0 # Only jet0 (pret)\n",
    "        ttag0 =   (~ttag_i0) & (~ttag_i1) # No tops tagged (0t)\n",
    "        ttag1 =   ttag_i0 ^ ttag_i1 # Exclusively one top tagged (1t)\n",
    "        ttagI =   ttag_i0 | ttag_i1 # At least one top tagged ('I' for 'inclusive' tagger; >=1t; 1t+2t)\n",
    "        ttag2 =   ttag_i0 & ttag_i1 # Both jets top tagged (2t)\n",
    "        Alltags = ttag0 | ttagI #Either no tag or at least one tag (0t+1t+2t)\n",
    "        \n",
    "        # ---- Pick FatJet that passes btag cut based on its subjet with the highest btag value ---- # \n",
    "        #btag_i0 = (ttbarcands.i0.btagCSVV2 > self.bdisc)\n",
    "        #btag_i1 = (ttbarcands.i1.btagCSVV2 > self.bdisc)\n",
    "        btag_i0 = ( np.maximum(SubJet01.btagCSVV2 , SubJet02.btagCSVV2) > self.bdisc )\n",
    "        btag_i1 = ( np.maximum(SubJet11.btagCSVV2 , SubJet12.btagCSVV2) > self.bdisc )\n",
    "        \n",
    "        # --- Define \"B Tag\" Regions ---- #\n",
    "        btag0 = (~btag_i0) & (~btag_i1) #(0b)\n",
    "        btag1 = btag_i0 ^ btag_i1 #(1b)\n",
    "        btag2 = btag_i0 & btag_i1 #(2b)\n",
    "        \n",
    "        # ---- Get Analysis Categories ---- # \n",
    "        # ---- They are (central, forward) cross (0b,1b,2b) cross (At,at,0t,1t,>=1t,2t) ---- #\n",
    "        regs = [cen,fwd]\n",
    "        btags = [btag0,btag1,btag2]\n",
    "        ttags = [antitag_probe,antitag,pretag,ttag0,ttag1,ttagI,ttag2,Alltags]\n",
    "        cats = [ (t&b&y).flatten() for t,b,y in itertools.product( ttags, btags, regs) ]\n",
    "        labels_and_categories = dict(zip( self.anacats, cats ))\n",
    "        \n",
    "        # ---- Variables for Kinematic Histograms ---- #\n",
    "        # ---- \"i0\" is the control jet, \"i1\" is the probe jet ---- #\n",
    "        ttbarmass = ttbarcands.p4.sum().mass.flatten()\n",
    "        jetpt = ttbarcands.i1.pt.flatten()\n",
    "        jeteta = ttbarcands.i1.eta.flatten()\n",
    "        jetphi = ttbarcands.i1.phi.flatten()\n",
    "        jety = ttbarcands.i1.p4.rapidity.flatten()\n",
    "        jetmass = ttbarcands.i1.p4.mass.flatten()\n",
    "        SDmass = ttbarcands.i1.msoftdrop.flatten()\n",
    "        jetdy = np.abs(ttbarcands.i0.p4.rapidity.flatten() - ttbarcands.i1.p4.rapidity.flatten())\n",
    "        Tau32 = (ttbarcands.i1.tau3/ttbarcands.i1.tau2).flatten()\n",
    "        # ---- Variables for Deep Tagger Analysis ---- #\n",
    "        deepTag = ttbarcands.i1.deepTag_TvsQCD.flatten()\n",
    "        deepTagMD = ttbarcands.i1.deepTagMD_TvsQCD.flatten()\n",
    "        \n",
    "        weights = evtweights.flatten()\n",
    "        \n",
    "        # ---- Define the SumW2 for MC Datasets ---- #\n",
    "        output['cutflow']['sumw'] += np.sum(weights)\n",
    "        output['cutflow']['sumw2'] += np.sum(weights**2)\n",
    "        \n",
    "        # ---- Define Momentum p of probe jet as the Mistag Rate variable; M(p) ---- #\n",
    "        # ---- Transverse Momentum pT can also be used instead; M(pT) ---- #\n",
    "        pT = ttbarcands.i1.pt.flatten()\n",
    "        eta = ttbarcands.i1.eta.flatten()\n",
    "        pz = np.sinh(eta)*pT\n",
    "        p = np.absolute(np.sqrt(pT**2 + pz**2))\n",
    "        \n",
    "        # ---- Define the Numerator and Denominator for Mistag Rate ---- #\n",
    "        numerator = np.where(antitag_probe, p, -1) # If no antitag and tagged probe, move event to useless bin\n",
    "        denominator = np.where(antitag, p, -1) # If no antitag, move event to useless bin\n",
    "        \n",
    "        df = pd.DataFrame({\"momentum\":p}) # Used for finding values in LookUp Tables\n",
    "        \n",
    "        for ilabel,icat in labels_and_categories.items():\n",
    "            ### ------------------------------------ Mistag Scaling ------------------------------------ ###\n",
    "            if self.UseLookUpTables == True:\n",
    "                # ---- Weight ttbar M.C. and data by mistag from data (corresponding to its year) ---- #\n",
    "                if 'TTbar_' in dataset:\n",
    "                    file_df = self.lu['JetHT' + dataset[-4:] + '_Data']['at' + str(ilabel[-5:])] #Pick out proper JetHT year mistag for TTbar sim.\n",
    "                elif dataset == 'TTbar':\n",
    "                    file_df = self.lu['JetHT']['at' + str(ilabel[-5:])] # All JetHT years mistag for TTbar sim.\n",
    "                else:\n",
    "                    file_df = self.lu[dataset]['at' + str(ilabel[-5:])] # get mistag (lookup) filename for 'at'\n",
    "                    \n",
    "                bin_widths = file_df['p'].values # collect bins as written in .csv file\n",
    "                mtr = file_df['M(p)'].values # collect mistag rate as function of p as written in file\n",
    "                wgts = mtr # Define weights based on mistag rates\n",
    "                \n",
    "                BinKeys = np.arange(bin_widths.size) # Use as label for BinNumber column in the new dataframe\n",
    "                \n",
    "                #Bins = pd.interval_range(start=0, periods=100, freq=100, closed='left') # Recreate the momentum bins from file_df as something readable for pd.cut()\n",
    "                Bins = np.array(manual_bins)\n",
    "                \n",
    "                df['BinWidth'] = pd.cut(p, bins=Bins) # new dataframe column\n",
    "                df['BinNumber'] = pd.cut(p, bins=Bins, labels=BinKeys)\n",
    "                #df['BinNumber'] = pd.cut(p, bins=Bins).map(dict(zip(Bins,BinKeys))) # Use if Bins is defined by pd.interval_range\n",
    "                \n",
    "                BinNumber = df['BinNumber'].values # Collect the Bin Numbers into a numpy array\n",
    "                BinNumber = BinNumber.astype('int64') # Insures the bin numbers are integers\n",
    "            \n",
    "                WeightMatching = wgts[BinNumber] # Match 'wgts' with corresponding p bin using the bin number\n",
    "                Weights = weights*WeightMatching # Include 'wgts' with the previously defined 'weights'\n",
    "            else:\n",
    "                Weights = weights # No mistag rates, no change to weights\n",
    "            ###---------------------------------------------------------------------------------------------###\n",
    "            ### ----------------------------------- Mod-mass Procedure ------------------------------------ ###\n",
    "            if self.ModMass == True:\n",
    "                QCD_unweighted = util.load('TTbarResCoffea_QCD_unweighted_output.coffea')\n",
    "\n",
    "                # ---- Extract event counts from QCD MC hist in signal region ---- #\n",
    "                QCD_hist = QCD_unweighted['jetmass'].integrate('anacat', '2t' + str(ilabel[-5:])).integrate('dataset', 'QCD')\n",
    "                data = QCD_hist.values() # Dictionary of values\n",
    "                QCD_data = [i for i in data.values()][0] # place every element of the dictionary into a numpy array\n",
    "\n",
    "                # ---- Re-create Bins from QCD_hist as Numpy Array ---- #\n",
    "                bins = np.arange(510) #Re-make bins from the jetmass_axis starting with the appropriate range\n",
    "                QCD_bins = bins[::10] #Finish re-making bins by insuring exactly 50 bins like the jetmass_axis\n",
    "\n",
    "                # ---- Define Mod Mass Distribution ---- #\n",
    "                ModMass_hist_dist = ss.rv_histogram([QCD_data,QCD_bins])\n",
    "                jet1_modp4 = copy.copy(jet1.p4) #J1's Lorentz four vector that can be safely modified\n",
    "                jet1_modp4[\"fMass\"] = ModMass_hist_dist.rvs(size=jet1_modp4.size) #Replace J1's mass with random value of mass from mm hist\n",
    "                ttbarcands_modmass = jet0.p4.cross(jet1_modp4) #J0's four vector x modified J1's four vector\n",
    "\n",
    "                # ---- Apply Necessary Selections to new modmass version ---- #\n",
    "                ttbarcands_modmass = ttbarcands_modmass[oneTTbar]\n",
    "                ttbarcands_modmass = ttbarcands_modmass[dPhiCut]\n",
    "                ttbarcands_modmass = ttbarcands_modmass[GoodSubjets]\n",
    "                \n",
    "                # ---- Manually sum the modmass p4 candidates (Coffea technicality) ---- #\n",
    "                ttbarcands_modmass_p4_sum = (ttbarcands_modmass.i0 + ttbarcands_modmass.i1)\n",
    "                \n",
    "                # ---- Re-define Mass Variables for ModMass Procedure (pt, eta, phi are redundant to change) ---- #\n",
    "                ttbarmass = ttbarcands_modmass_p4_sum.flatten().mass\n",
    "                jetmass = ttbarcands_modmass.i1.mass.flatten()\n",
    "            ###---------------------------------------------------------------------------------------------###\n",
    "            output['cutflow'][ilabel] += np.sum(icat)\n",
    "          \n",
    "            output['ttbarmass'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                ttbarmass=ttbarmass[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jetpt'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetpt=jetpt[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['probept'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetpt=pT[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['probep'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetp=p[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jeteta'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jeteta=jeteta[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jetphi'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetphi=jetphi[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jety'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jety=jety[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jetdy'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetdy=jetdy[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['numerator'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetp=numerator[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['denominator'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                jetp=denominator[icat],\n",
    "                                weight=Weights[icat])\n",
    "            output['jetmass'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                   jetmass=jetmass[icat],\n",
    "                                   weight=Weights[icat])\n",
    "            output['SDmass'].fill(dataset=dataset, anacat=ilabel, \n",
    "                                   jetmass=SDmass[icat],\n",
    "                                   weight=Weights[icat])\n",
    "            output['tau32'].fill(dataset=dataset, anacat=ilabel,\n",
    "                                          tau32=Tau32[icat],\n",
    "                                          weight=Weights[icat])\n",
    "            output['tau32_2D'].fill(dataset=dataset, anacat=ilabel,\n",
    "                                          jetpt=pT[icat],\n",
    "                                          tau32=Tau32[icat],\n",
    "                                          weight=Weights[icat])\n",
    "            output['deepTag_TvsQCD'].fill(dataset=dataset, anacat=ilabel,\n",
    "                                          jetpt=pT[icat],\n",
    "                                          tagger=deepTag[icat],\n",
    "                                          weight=Weights[icat])\n",
    "            output['deepTagMD_TvsQCD'].fill(dataset=dataset, anacat=ilabel,\n",
    "                                            jetpt=pT[icat],\n",
    "                                            tagger=deepTagMD[icat],\n",
    "                                            weight=Weights[icat])\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesets = {\n",
    "    'ZprimeDM':zprimeDMfiles,\n",
    "    'TTbar':ttbarfiles,\n",
    "    'QCD':qcdfiles,\n",
    "    'JetHT':jetdatafiles,\n",
    "    'JetHT2016_Data':jetdatafiles2016,\n",
    "    'JetHT2017_Data':jetdatafiles2017,\n",
    "    'JetHT2018_Data':jetdatafiles2018\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tstart = time.time()\n",
    "\n",
    "outputs_unweighted = {}\n",
    "\n",
    "seed = 1234567890\n",
    "prng = RandomState(seed)\n",
    "Chunk = [100000, 600] # [chunksize, maxchunks]\n",
    "\n",
    "for name,files in filesets.items(): \n",
    "    \n",
    "\n",
    "    print(name)\n",
    "    output = processor.run_uproot_job({name:files},\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=TTbarResProcessor(UseLookUpTables=False,\n",
    "                                                                           ModMass=False,\n",
    "                                                                           RandomDebugMode=False,\n",
    "                                                                           prng=prng),\n",
    "                                      executor=processor.dask_executor,\n",
    "                                      #executor=processor.iterative_executor,\n",
    "                                      #executor=processor.futures_executor,\n",
    "                                      executor_args={\n",
    "                                          'client': client, \n",
    "                                          'nano':False, \n",
    "                                          'flatten':True, \n",
    "                                          'skipbadfiles':False,\n",
    "                                          'workers': 2},\n",
    "                                      chunksize=Chunk[0], maxchunks=Chunk[1]\n",
    "                                     )\n",
    "\n",
    "    elapsed = time.time() - tstart\n",
    "    outputs_unweighted[name] = output\n",
    "    print(output)\n",
    "    util.save(output, 'TTbarResCoffea_' + name + '_unweighted_output_partial_2021_dask_run.coffea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Elapsed time = ', elapsed, ' sec.')\n",
    "print('Elapsed time = ', elapsed/60., ' min.')\n",
    "print('Elapsed time = ', elapsed/3600., ' hrs.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name,output in outputs_unweighted.items(): \n",
    "    print(\"-------Unweighted \" + name + \"--------\")\n",
    "    for i,j in output['cutflow'].items():        \n",
    "        print( '%20s : %12d' % (i,j) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(mypath):\n",
    "    '''Creates a directory. equivalent to using mkdir -p on the command line'''\n",
    "\n",
    "    from errno import EEXIST\n",
    "    from os import makedirs,path\n",
    "\n",
    "    try:\n",
    "        makedirs(mypath)\n",
    "    except OSError as exc: # Python >2.5\n",
    "        if exc.errno == EEXIST and path.isdir(mypath):\n",
    "            pass\n",
    "        else: raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoesDirectoryExist(mypath): #extra precaution (Probably overkill...)\n",
    "    '''Checks to see if Directory exists before running mkdir_p'''\n",
    "    import os.path\n",
    "    from os import path\n",
    "    \n",
    "    if path.exists(mypath):\n",
    "        pass\n",
    "    else:\n",
    "        mkdir_p(mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import re # regular expressions\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- Reiterate categories ---- #\n",
    "ttagcats = [\"at\"] #, \"0t\", \"1t\", \"It\", \"2t\"]\n",
    "btagcats = [\"0b\", \"1b\", \"2b\"]\n",
    "ycats = ['cen', 'fwd']\n",
    "\n",
    "list_of_cats = [ t+b+y for t,b,y in itertools.product( ttagcats, btagcats, ycats) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maindirectory = os.getcwd() # changes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ---------------- CREATES MISTAG PLOTS ---------------- \"\"\"\n",
    "# ---- Only Use This Cell When LookUp Tables Are Not In Use (i.e. UseLookUpTables = False) ---- #\n",
    "\n",
    "SaveDirectory = maindirectory + '/MistagPlots/'\n",
    "DoesDirectoryExist(SaveDirectory) # no need to create the directory several times\n",
    "\n",
    "# Function sqrt(x)\n",
    "def forward(x):\n",
    "    return x**(1/2)\n",
    "\n",
    "\n",
    "def inverse(x):\n",
    "    return x**2\n",
    "\n",
    "print(SaveDirectory)\n",
    "for iset in filesets:\n",
    "    for icat in list_of_cats:\n",
    "        print(iset)\n",
    "        print(icat)\n",
    "        title = iset + ' mistag ' + icat\n",
    "        filename = 'mistag_' + iset + '_' + icat + '.' + 'png'\n",
    "        print(outputs_unweighted[iset]['numerator'])\n",
    "        Numerator = outputs_unweighted[iset]['numerator'].integrate('anacat', icat).integrate('dataset', iset)\n",
    "        Denominator = outputs_unweighted[iset]['denominator'].integrate('anacat', icat).integrate('dataset', iset)\n",
    "        print(Numerator)\n",
    "        print(Denominator)\n",
    "        mistag = hist.plotratio(num = Numerator, denom = Denominator,\n",
    "                                error_opts={'marker': '.', 'markersize': 10., 'color': 'k', 'elinewidth': 1},\n",
    "                                unc = 'num')\n",
    "        plt.title(title)\n",
    "        plt.ylim(bottom = 0, top = 0.12)\n",
    "        plt.xlim(left = 100, right = 2500)\n",
    "        \n",
    "        # ---------- Better mistag plots are made in separate notebook ----------- #\n",
    "        # ---- However, if one wants to save these plots right here, they may ---- #\n",
    "        \n",
    "        #plt.xticks(np.array([0, 500, 600, 700]))\n",
    "        #mistag.set_xscale('function', functions=(forward, inverse))\n",
    "        #mistag.set_xscale('log')\n",
    "        #plt.savefig(SaveDirectory+filename, bbox_inches=\"tight\")\n",
    "        #print(filename + ' saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ---------------- Scale-Factors for JetHT Data According to Year---------------- \"\"\"\n",
    "Nevts2016 = 625516390. # from dasgoclient\n",
    "#Nevts2016_totalchunks = Nevts2016 / Chunk[0]\n",
    "Nevts2016_sf = Nevts2016/outputs_unweighted['JetHT2016_Data']#Nevts2016_totalchunks / Chunk[1]\n",
    "\n",
    "Nevts2017 = 410461585. # from dasgoclient\n",
    "#Nevts2017_totalchunks = Nevts2017 / Chunk[0]\n",
    "Nevts2017_sf = Nevts2017/outputs_unweighted['JetHT2017_Data']#Nevts2017_totalchunks / Chunk[1]\n",
    "\n",
    "Nevts2018 = 676328827. # from dasgoclient\n",
    "#Nevts2018_totalchunks = Nevts2018 / Chunk[0]\n",
    "Nevts2018_sf = Nevts2018/outputs_unweighted['JetHT2018_Data']#Nevts2018_totalchunks / Chunk[1]\n",
    "\n",
    "Nevts = Nevts2016 + Nevts2017 + Nevts2018\n",
    "Nevts_totalchunks = Nevts / Chunk[0]\n",
    "Nevts_sf = Nevts_totalchunks / Chunk[1]\n",
    "\n",
    "print(Nevts2016_sf)\n",
    "print(Nevts2017_sf)\n",
    "print(Nevts2018_sf)\n",
    "print(Nevts_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ---------------- Luminosities, Cross Sections, Scale-Factors ---------------- \"\"\" \n",
    "Lum2016 = 35920. # pb^-1 from https://twiki.cern.ch/twiki/bin/viewauth/CMS/PdmVAnalysisSummaryTable\n",
    "Lum2017 = 41530.\n",
    "Lum2018 = 59740.\n",
    "Lum     = 137190.\n",
    "\n",
    "ttbar_BR = 0.457 # 0.442 from PDG 2018\n",
    "ttbar_xs = 1.0   # Monte Carlo already includes xs in event weight!! Otherwise, ttbar_xs = 831.76 * ttbar_BR  pb\n",
    "\n",
    "ttbar2016_sf = ttbar_xs*Lum2016/(142155064.)\n",
    "ttbar2017_sf = ttbar_xs*Lum2017/(142155064.)\n",
    "ttbar2018_sf = ttbar_xs*Lum2018/(142155064.)\n",
    "ttbar_sf = ttbar_xs*Lum/(142155064.)\n",
    "\n",
    "print(ttbar2016_sf)\n",
    "print(ttbar2017_sf)\n",
    "print(ttbar2018_sf)\n",
    "print(ttbar_sf)\n",
    "\n",
    "qcd_xs = 1370000000.0 #pb From https://cms-gen-dev.cern.ch/xsdb\n",
    "#qcd_sf = qcd_xs*Lum/18455107."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ---------------- CREATE LOOK UP TABLE .CSV FILES ---------------- \"\"\"\n",
    "# ---- Only Use This Cell When LookUp Tables Are Not In Use for Previous Cell (UseLookUpTables = False) ---- #\n",
    "from collections import defaultdict\n",
    "\n",
    "runLUTS = True # Make separate Directory to place Look-Up Tables and perform ttbar subtraction for mistag weights\n",
    "\n",
    "def multi_dict(K, type): # definition from https://www.geeksforgeeks.org/python-creating-multidimensional-dictionary/\n",
    "    if K == 1: \n",
    "        return defaultdict(type) \n",
    "    else: \n",
    "        return defaultdict(lambda: multi_dict(K-1, type))\n",
    "    \n",
    "luts = {}\n",
    "luts = multi_dict(2, str)\n",
    "\n",
    "if runLUTS : \n",
    "\n",
    "    SaveDirectory = maindirectory + '/LookupTables/'\n",
    "    DoesDirectoryExist(SaveDirectory)\n",
    "    \n",
    "    # ---- Check if TTbar simulation was used in previous processor ---- #\n",
    "    if 'TTbar' in filesets:\n",
    "        for iset in filesets:\n",
    "            if iset != 'TTbar' or iset != 'QCD': # if JetHT filesets are found...\n",
    "                print('\\t\\tfileset: ' + iset + '\\n*****************************************************\\n')\n",
    "                for icat in list_of_cats:\n",
    "                    title = iset + ' mistag ' + icat\n",
    "                    filename = 'mistag_' + iset + '_' + icat + '.' + 'csv'\n",
    "\n",
    "                    # ---- Info from TTbar ---- #\n",
    "                    Numerator_tt = outputs_unweighted['TTbar']['numerator'].integrate('anacat',icat).integrate('dataset','TTbar')\n",
    "                    Denominator_tt = outputs_unweighted['TTbar']['denominator'].integrate('anacat',icat).integrate('dataset','TTbar')\n",
    "                    N_vals_tt = Numerator_tt.values()[()] #np.zeros( np.size(Numerator_tt.values()[()]) ) \n",
    "                    D_vals_tt = Denominator_tt.values()[()] #np.zeros( np.size(Denominator_tt.values()[()]) ) \n",
    "\n",
    "                    # ---- Info from JetHT datasets ---- #\n",
    "                    Numerator = outputs_unweighted[iset]['numerator'].integrate('anacat',icat).integrate('dataset',iset)\n",
    "                    Denominator = outputs_unweighted[iset]['denominator'].integrate('anacat',icat).integrate('dataset',iset)\n",
    "                    N_vals = Numerator.values()[()]\n",
    "                    D_vals = Denominator.values()[()]\n",
    "\n",
    "                    # ---- Properly scale chunks of data and ttbar MC according to year of dataset used---- #\n",
    "                    if '2016' in iset:\n",
    "                        N_vals *= Nevts2016_sf \n",
    "                        D_vals *= Nevts2016_sf\n",
    "                        N_vals_tt *= ttbar2016_sf\n",
    "                        D_vals_tt *= ttbar2016_sf\n",
    "                    elif '2017' in iset:\n",
    "                        N_vals *= Nevts2017_sf \n",
    "                        D_vals *= Nevts2017_sf\n",
    "                        N_vals_tt *= ttbar2017_sf\n",
    "                        D_vals_tt *= ttbar2017_sf\n",
    "                    elif '2018' in iset:\n",
    "                        N_vals *= Nevts2018_sf \n",
    "                        D_vals *= Nevts2018_sf\n",
    "                        N_vals_tt *= ttbar2018_sf\n",
    "                        D_vals_tt *= ttbar2018_sf\n",
    "                    else: # all years\n",
    "                        N_vals *= Nevts_sf \n",
    "                        D_vals *= Nevts_sf\n",
    "                        N_vals_tt *= ttbar_sf\n",
    "                        D_vals_tt *= ttbar_sf\n",
    "\n",
    "                    # ---- Subtract ttbar MC probe momenta from datasets' ---- #\n",
    "                    N_vals_diff = np.abs(N_vals-N_vals_tt)\n",
    "                    D_vals_diff = np.abs(D_vals-D_vals_tt)\n",
    "\n",
    "                    print(N_vals_diff)\n",
    "                    print(D_vals_diff)\n",
    "                    print()\n",
    "\n",
    "                    # ---- Define Mistag values ---- #\n",
    "                    mistag_vals = np.where(D_vals_diff > 0, N_vals_diff/D_vals_diff, 0)\n",
    "                    \n",
    "                    # ---- Define Momentum values ---- #\n",
    "                    p_vals = []\n",
    "                    for iden in Numerator.identifiers('jetp'):\n",
    "                        p_vals.append(iden)\n",
    "\n",
    "                    # ---- Display and Save Dataframe, df, as Look-up Table ---- #\n",
    "                    print('fileset:  ' + iset)\n",
    "                    print('category: ' + icat)\n",
    "                    print('________________________________________________\\n')\n",
    "\n",
    "                    d = {'p': p_vals, 'M(p)': mistag_vals} # 'data'\n",
    "\n",
    "                    print(\"d vals = \", d)\n",
    "                    print()\n",
    "                    df = pd.DataFrame(data=d)\n",
    "                    luts[iset][icat] = df\n",
    "\n",
    "                    with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
    "                        print(df)\n",
    "                    print('\\n')\n",
    "\n",
    "                    df.to_csv(SaveDirectory+filename) # use later to collect bins and weights for re-scaling\n",
    "            else: # If iset is not JetHT...\n",
    "                for icat in list_of_cats:\n",
    "                    Numerator = outputs_unweighted[iset]['numerator'].integrate('anacat',icat).integrate('dataset',iset)\n",
    "                    Denominator = outputs_unweighted[iset]['denominator'].integrate('anacat',icat).integrate('dataset',iset)\n",
    "                    N_vals = Numerator.values()[()]\n",
    "                    D_vals = Denominator.values()[()]\n",
    "                    print(N_vals)\n",
    "                    print(D_vals)\n",
    "                    print()\n",
    "                    mistag_vals = np.where(D_vals > 0, N_vals/D_vals, 0)\n",
    "\n",
    "                    p_vals = [] # Momentum values\n",
    "                    for iden in Numerator.identifiers('jetp'):\n",
    "                        p_vals.append(iden)\n",
    "                    print('fileset:  ' + iset)\n",
    "                    print('category: ' + icat)\n",
    "                    print('________________________________________________\\n')\n",
    "                    d = {'p': p_vals, 'M(p)': mistag_vals}\n",
    "\n",
    "                    print(\"d vals = \", d)\n",
    "                    print()\n",
    "                    df = pd.DataFrame(data=d)\n",
    "                    luts[iset][icat] = df\n",
    "\n",
    "                    with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
    "                        print(df)\n",
    "                    print('\\n')\n",
    "\n",
    "                    df.to_csv(SaveDirectory+filename) # use later to collect bins and weights for re-scaling\n",
    "\n",
    "    else: # If iset did not run over 'TTbar' Simulation...\n",
    "        for iset in filesets:\n",
    "            print('\\t\\tfileset: ' + iset + '\\n*****************************************************\\n')\n",
    "            for icat in list_of_cats:\n",
    "                Numerator = outputs_unweighted[iset]['numerator'].integrate('anacat',icat).integrate('dataset',iset)\n",
    "                Denominator = outputs_unweighted[iset]['denominator'].integrate('anacat',icat).integrate('dataset',iset)\n",
    "                N_vals = Numerator.values()[()]\n",
    "                D_vals = Denominator.values()[()]\n",
    "                print(N_vals)\n",
    "                print(D_vals)\n",
    "                print()\n",
    "                \n",
    "                mistag_vals = np.where(D_vals > 0, N_vals/D_vals, 0)\n",
    "\n",
    "                p_vals = []\n",
    "                for iden in Numerator.identifiers('jetp'):\n",
    "                    p_vals.append(iden)\n",
    "                    \n",
    "                print('fileset:  ' + iset)\n",
    "                print('category: ' + icat)\n",
    "                print('________________________________________________\\n')\n",
    "                d = {'p': p_vals, 'M(p)': mistag_vals}\n",
    "\n",
    "                print(\"d vals = \", d)\n",
    "                print()\n",
    "                df = pd.DataFrame(data=d)\n",
    "                luts[iset][icat] = df\n",
    "\n",
    "                with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
    "                    print(df)\n",
    "                print('\\n')\n",
    "\n",
    "                df.to_csv(SaveDirectory+filename) # use later to collect bins and weights for re-scaling\n",
    "            \n",
    "else : # If runLUTS = False...\n",
    "    for iset in filesets:\n",
    "        print('\\t\\tfileset: ' + iset + '\\n*****************************************************\\n')\n",
    "        for icat in list_of_cats:\n",
    "            title = iset + ' mistag ' + icat\n",
    "            filename = 'mistag_' + iset + '_' + icat + '.' + 'csv'\n",
    "            luts[iset][icat] = pd.read_csv(filename)\n",
    "print(luts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Events/s:\", output['cutflow']['all events']/elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesets = {\n",
    "    'ZprimeDM':zprimeDMfiles,\n",
    "    'TTbar_2016':ttbarfiles, # TTbar MC to be weighted with mistag from JetHT2016_Data\n",
    "    'TTbar_2017':ttbarfiles, # TTbar MC to be weighted with mistag from JetHT2017_Data\n",
    "    'TTbar_2018':ttbarfiles, # TTbar MC to be weighted with mistag from JetHT2018_Data\n",
    "    'TTbar':ttbarfiles,\n",
    "    'QCD':qcdfiles,\n",
    "    'JetHT':jetdatafiles,\n",
    "    'JetHT2016_Data':jetdatafiles2016,\n",
    "    'JetHT2017_Data':jetdatafiles2017,\n",
    "    'JetHT2018_Data':jetdatafiles2018\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Runs Processor, Weights Datasets with Corresponding Mistag Weight \"\"\"\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "outputs_weighted = {}\n",
    "prng = RandomState(seed)\n",
    "Chunk = [100000, 600] # [chunksize, maxchunks]\n",
    "\n",
    "for name,files in filesets.items(): \n",
    "    \n",
    "\n",
    "    print(name)\n",
    "    output = processor.run_uproot_job({name:files},\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=TTbarResProcessor(UseLookUpTables=True,\n",
    "                                                                           ModMass = False,\n",
    "                                                                           lu=luts,\n",
    "                                                                           prng=prng),\n",
    "                                      executor=processor.dask_executor,\n",
    "                                      #executor=processor.iterative_executor,\n",
    "                                      #executor=processor.futures_executor,\n",
    "                                      executor_args={\n",
    "                                          'client': client, \n",
    "                                          'nano':False, \n",
    "                                          'flatten':True, \n",
    "                                          'skipbadfiles':False,\n",
    "                                          'workers': 2}\n",
    "                                      #chunksize=Chunk[0], maxchunks=Chunk[1]\n",
    "                                     )\n",
    "\n",
    "    elapsed = time.time() - tstart\n",
    "    outputs_weighted[name] = output\n",
    "    print(output)\n",
    "    util.save(output, 'TTbarResCoffea_' + name + '_weighted_output_partial_2021_dask_run.coffea')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name,output in outputs_weighted.items(): \n",
    "    print(\"-------weighted \" + name + \"--------\")\n",
    "    for i,j in output['cutflow'].items():        \n",
    "        print( '%20s : %12d' % (i,j) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Runs Processor, Weights Datasets with Corresponding Mistag Weight, Implements Mass Modification Procedure \"\"\"\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "outputs_weighted = {}\n",
    "prng = RandomState(seed)\n",
    "Chunk = [100000, 600] # [chunksize, maxchunks]\n",
    "\n",
    "for name,files in filesets.items(): \n",
    "    \n",
    "\n",
    "    print(name)\n",
    "    output = processor.run_uproot_job({name:files},\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=TTbarResProcessor(UseLookUpTables=True,\n",
    "                                                                           ModMass = True,\n",
    "                                                                           lu=luts,\n",
    "                                                                           prng=prng),\n",
    "                                      executor=processor.dask_executor,\n",
    "                                      #executor=processor.iterative_executor,\n",
    "                                      #executor=processor.futures_executor,\n",
    "                                      executor_args={\n",
    "                                         # 'client': client, \n",
    "                                          'nano':False, \n",
    "                                          'flatten':True, \n",
    "                                          'skipbadfiles':False,\n",
    "                                          'workers': 2}\n",
    "                                      #chunksize=Chunk[0], maxchunks=Chunk[1]\n",
    "                                     )\n",
    "\n",
    "    elapsed = time.time() - tstart\n",
    "    outputs_weighted[name] = output\n",
    "    print(output)\n",
    "    util.save(output, 'TTbarResCoffea_' + name + '_ModMass_weighted_output_partial_2021_dask_run.coffea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
